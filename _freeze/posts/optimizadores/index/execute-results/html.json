{
  "hash": "b05fec7dc5d393aa23fa80b890284c2f",
  "result": {
    "markdown": "---\ntitle: \"Optimizadores en torch\"\nauthor: \"Jhonatan Zambrano\"\ndate: \"2023-03-16\"\ncategories: [torch, optimizadores]\nabstract: \"Aqui concluye la mini-serie de bases de torch, adicionando a nuestra caja de herramientas dos abstracciones: funciones de perdidas y optimizadores\"\nimage: \"optim.png\"\nlang: \"es\"\ndraft: false\n---\n\n\n## Introducción\n\n![](optim.png)\n\nEsta es la cuarta y última entrega de una serie que presenta las bases de `torch`. Inicialmente, no enfocamos en los tensores. Para ilustrar su potencia, codificamos una red neuronal completa (aunque de pequeño tamaño) desde cero. Allí no se usó ninguna de las capacidades de alto nivel de `torch`, ni siquiera `autograd`, su herramienta de diferenciación automática.\n\nEsto cambio en la siguiente entrega. No seguimos pensando en derivadas o en la regla de la cadena; un llamado a `backward()` fue suficiente.\n\nEn la tercera entrega, el código vio nuevamente una simplificación importante. En lugar del tedioso ensamble del grafo (disposición de las capas) manualmente, se dejó que los *módulos* se encargaran.\n\nPartiendo de lo anterior, quedan dos cosas mas por hacer. Primero, aún calculamos las perdidas a mano. Segundo, aunque obtenemos los gradientes buenamente calculados de `autograd`, aún programamos un ciclo sobre los parámetros para actualizarlos por nuestros propios medios. No es una sorpresa saber que nada de esto es necesario.\n\n## Perdidas y funciones de perdidas\n\n`torch` incluye todas las funciones usuales de perdidas, tales como *error cuadrático medio*, *entropía cruzada*, *divergencia Kullback-Leibler* y similares. En general, hay dos modos de uso.\n\nTomemos por ejemplo el calculo del error cuadratico medio. Una manera es invocando `nnf_mse_loss()` directamente en la predicción y los valores de salida verdaderos:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nx <- torch_randn(c(3, 2, 3))\ny <- torch_zeros(c(3, 2, 3))\n\nnnf_mse_loss(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1.20262\n[ CPUFloatType{} ]\n```\n:::\n:::\n\n\nOtras funciones de perdidas designadas para ser invocadas directamente inician con `nnf_` como: `nnf_binary_cross_entropy()`, `nnf_nll_loss()`, `nnf_kl_div()` y asi sucesivamente [^1].\n\n[^1]: El prefijo nnf_ fue escogido porque en `PyTorch`, las funciones correspondientes *viven* en `torch.nn.functional`.\n\nLa segunda forma es definir el algoritmo previamente e invocarlo posteriormente. En este caso, todos los constructores inician con `nn_` y terminan en `_loss`. Por ejemplo: `nn_bce_loss()`, `nn_nll_loss()`, `nn_kl_div_loss()`, etc [^3].\n\n[^3]: Esta vez, el módulo correspondiente de `PyTorch` es `torch.nn`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss <- nn_mse_loss()\n\nloss(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1.20262\n[ CPUFloatType{} ]\n```\n:::\n:::\n\n\nEl último método es preferido cuando el mismo único algoritmo debe ser aplicado a mas de un par de tensores.\n\n## Optimizadores\n\nHasta ahora, hemos estado actualizando los parámetros del modelo usando una estrategia simple: Los gradientes nos indican en que dirección la curva de la función de perdidas va hacia abajo; la rata de aprendizaje nos dice que tan grande debe ser el paso que se de en dicha dirección. Lo que hicimos fue una implementación directa del *descenso del gradiente*.\n\nSin embargo, los algoritmos de optimización usado en *aprendizaje profundo* son mucho mas sofisticados. Abajo, observaremos como reemplazar nuestras actualizaciones manuales usando el algoritmo *Adam* (Kingma y Ba 2017). Aunque primero, demos un vistazo a cómo trabajan los optimizadores de `torch`.\n\nAquí tenemos una red muy sencilla que consiste en una sola capa lineal a ser invocada por un único punto de datos (una salida).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- torch_randn(1, 3)\n\nmodel <- nn_linear(3, 1)\nmodel$parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nCuando se crea un optimizador, le estamos diciendo que parámetros deben ser usados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer <- optim_adam(model$parameters, lr = 0.01)\noptimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<optim_adam>\n  Inherits from: <torch_optimizer>\n  Public:\n    add_param_group: function (param_group) \n    clone: function (deep = FALSE) \n    defaults: list\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    load_state_dict: function (state_dict) \n    param_groups: list\n    state: State, R6\n    state_dict: function () \n    step: function (closure = NULL) \n    zero_grad: function () \n  Private:\n    step_helper: function (closure, loop_fun) \n```\n:::\n:::\n\n\nEn cualquier momento podemos inspeccionar estos parámetros:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer$param_groups[[1]]$params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nAhora vamos a realizar la propagación hacia adelante y hacia atrás. La retro-propagación calculará los gradientes, pero *no* actualiza los parámetros, como podemos ver a continuación de los objetos `model` y `optimizer`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- model(data)\nout$backward()\n\noptimizer$param_groups[[1]]$params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n\n```{.r .cell-code}\nmodel$parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nInvocando el método `step()` en el optimizador se realiza la actualización de los pesos del modelo. De nuevo, revisemos que, tanto `model` como `optimizer`, ahora contienen los valores actualizados:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer$step()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\noptimizer$param_groups[[1]]$params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.4208  0.3039  0.4202\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5642\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n\n```{.r .cell-code}\nmodel$parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.4208  0.3039  0.4202\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5642\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nSi realizamos la optimización en un ciclo, necesitamos asegurarnos de que la invocación a `optimizer$zero_grad() en cada paso, porque de otro modo los gradientes se acumularían. Podemos ahora ver la versión final de nuestra red neuronal.\n\n## Red neuronal simple: la version final\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### generación de datos de entrenamiento ---------------\n\n# dimensiones de la entrada (número de características de entrada)\nd_in <- 3\n# dimensiones de la salida (número de características de predicción)\nd_out <- 1\n# número de observaciones en el conjunto de entrenamiento\nn <- 100\n\n# Creación de datos aleatorios\nx <- torch_randn(n, d_in)\ny <- x[, 1, drop = F] * 0.2 - x[, 2, drop = F] * 1.3 - x[, 3, drop = F] * 0.5 + torch_randn(n, 1)\n\n### Definición de la red neuronal\n\n# dimensiones de la capa oculta\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### Parámetros de la red\n\n# para optimización Adam, necesitamos escoger una tasa de aprendizaje mas alta en este caso\nlearning_rate <- 0.08\n\noptimizer <- optim_adam(model$parameters, lr = learning_rate)\n\n### Ciclo de entrenamiento\n\nfor (t in 1:200){\n  ### ------ propagación hacia adelante---------\n  \n  y_pred <- model(x)\n  \n  ### ------ cálculo de perdidas --------\n  \n  loss <- nnf_mse_loss(y_pred, y, reduction = \"sum\")\n  if(t %% 10 ==0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### ------- retro-propagación ---------\n  \n  # puesta a cero de los gradientes antes de iniciar la retro-propagación\n  optimizer$zero_grad()\n  \n  # Cálculo de los gradientes para los parámetros del modelo\n  loss$backward()\n  \n  ### ------- actualización de los pesos -------\n  \n  # se usa el optimizador para actualizar los parámetros del modelo\n  optimizer$step()\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  10    Loss:  101.9358 \nEpoch:  20    Loss:  83.14806 \nEpoch:  30    Loss:  76.63703 \nEpoch:  40    Loss:  71.65172 \nEpoch:  50    Loss:  65.41241 \nEpoch:  60    Loss:  59.46485 \nEpoch:  70    Loss:  55.67398 \nEpoch:  80    Loss:  52.05198 \nEpoch:  90    Loss:  50.23497 \nEpoch:  100    Loss:  48.91325 \nEpoch:  110    Loss:  45.77558 \nEpoch:  120    Loss:  44.74579 \nEpoch:  130    Loss:  41.58841 \nEpoch:  140    Loss:  49.05213 \nEpoch:  150    Loss:  42.33701 \nEpoch:  160    Loss:  38.36844 \nEpoch:  170    Loss:  38.33907 \nEpoch:  180    Loss:  36.47657 \nEpoch:  190    Loss:  35.86893 \nEpoch:  200    Loss:  36.88996 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}