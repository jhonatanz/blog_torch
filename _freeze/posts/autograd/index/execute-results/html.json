{
  "hash": "72b8900b698566217e9851729e804afa",
  "result": {
    "markdown": "---\ntitle: \"Presentando el autograd de torch\"\nauthor: \"Jhonatan Zambrano\"\ndate: \"2023-02-14\"\ncategories: [torch, autograd]\nabstract: \"Con torch, es dificil encontrar una razon para programar desde el principio la retro-propagación. Su caracteristica de diferenciación automática, llamada autograd, lleva un registro de las operaciones que sus computos del gradiente requiere, asi como tambien el cómo calcularlos. En esta segunda parte de una serie de cuatro, actualizamos nuestra red neuronal simple de la primera entrega para hacer uso de autograd.\"\nimage: \"grad.png\"\nlang: \"es\"\n---\n\n\n\n\n## Introducción\n\n![](grad.png)\n\nAnteriormente se vio como programar una red neuronal simple desde el principio usando únicamente tensores de `torch`. Las predicciones, perdidas, gradientes, actualizaciones en los pesos, todas estas cosas las calculamos nosotros mismos. Ahora vamos a hacer un avance significativo: vamos a ahorrarnos el calculo de los gradientes y tenemos a `torch` para que haga eso por nosotros.\n\nPero antes, debemos obtener un poco de contexto.\n\n## Diferenciación automatica con autograd\n\n`torch` usa un módulo llamado `autograd` para:\n\n1.  Registrar las operaciones aplicadas a los tensores y\n2.  Almacenar lo que se ha hecho para obtener los gradientes correspondientes, una vez que se ha entrado en la fase de retro-propagación.\n\nEstas acciones previas son almacenadas internamente como funciones y cuando es el momento de calcular los gradientes, estas funciones se aplican en orden: se inicia desde el nodo de salida y se calculan los gradientes sucesivamente en retro-propagación a través de la red. Es una forma de *modo reverso de diferenciación automática*.\n\n### Bases del autograd\n\nComo usuarios, podemos ver un poco de la implementación. Como prerequisito para que el *registro* ocurra, los tensores tiene que ser creados con `requires_grad = TRUE`. Por ejemplo:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nx <- torch_ones(2, 2, requires_grad = TRUE)\n```\n:::\n\n\nPara ser claros, `x` es ahora un tensor *con respecto al cual* se tiene que calcular el gradiente, normalmente un tensor que representa pesos o sesgos [^1], no los datos de entrada. Si nosotros aplicamos subsecuentemente una operación a dicho tensor y asignamos el resultado a `y`:\n\n[^1]: En realidad el concepto es mucho mas general, `x` es un tensor que representa un punto del espacio del dominio de una función, para el cual `torch` va a calcular el respectivo gradiente de la función en dicho punto.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- x$mean()\n```\n:::\n\n\nAhora encontramos que y tiene un registro no-vacio en `grad_fn` que indica como debe calcularse el gradiente de `y` en el punto `x`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny$grad_fn\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeanBackward0\n```\n:::\n:::\n\n\nEl cálculo de los gradientes se hace en el llamado a la función `backward()` en el tensor de salida.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny$backward()\n```\n:::\n\n\nDespués de `backward()`, `x` tiene un campo no-nulo llamado `grad`que almacena el gradiente de `y` en el punto `x`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\n```\n:::\n:::\n\n\nEn cadenas mas largas de cómputos, podemos revisar como `torch` construye un grafo de operaciones `backward`. A continuación tenemos un ejemplo un poco mas complejo, siéntase libre de saltarlo si no es del tipo que inspeccionar los detalles para que la cosas tengan sentido.\n\n### Profundizando\n\nSe construye un grafo simple de tensores, con entradas `x1`y `x2` siendo conectadas a la salida `out` por intermedio de `y` y `z`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- torch_ones(2, 2, requires_grad = TRUE)\nx2 <- torch_tensor(1.1, requires_grad = TRUE)\n\ny <- x1*(x2+2)\nz <- y$pow(2)*3\nout <- z$mean()\n```\n:::\n\n\nPara ahorrar memoria, los gradientes intermedios normalmente no son almacenados. Llamando a `retain_grad()` en un tensor nos permite cambiar la opción predeterminada. Hagamoslo aquí a modo de demostración:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny$retain_grad()\nz$retain_grad()\n```\n:::\n\n\nAhora podemos revisar el grafo para inspeccionar el plan de acción de `torch` para la retro-progagación, empezando desde `out$grad_fn`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cómo se calcula el gradiente para el promedio, la ultima acción ejecutada\nout$grad_fn\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeanBackward0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cómo se calcula el gradiente para la multiplicación por 3 en z = y.pow(2)*3\nout$grad_fn$next_functions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nMulBackward1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cómo se calcula el gradiente para pow en z = y.pow(2)*3\nout$grad_fn$next_functions[[1]]$next_functions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nPowBackward0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cómo se calcula el gradiente para la multiplicación en y = x*(x+2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nMulBackward0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cómo se calcula el gradiente de las dos ramas de y = x*(x+2), donde el camino izquierdo es un nodo hoja (AccumulateGrad para x1)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntorch::autograd::AccumulateGrad\n[[2]]\nAddBackward1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Aquí llegamos a otro nodo hoja (AccumulateGrad para x2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions[[2]]$next_functions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntorch::autograd::AccumulateGrad\n```\n:::\n:::\n\n\nSi llamamos ahora `out$backward()`, todos los tensores en el grafo tendrán sus respectivos gradientes calculados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$backward()\n\nz$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\n```\n:::\n\n```{.r .cell-code}\ny$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 4.6500  4.6500\n 4.6500  4.6500\n[ CPUFloatType{2,2} ]\n```\n:::\n\n```{.r .cell-code}\nx2$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 18.6000\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nx1$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 14.4150  14.4150\n 14.4150  14.4150\n[ CPUFloatType{2,2} ]\n```\n:::\n:::\n\n\nDespués de esta revisión, observemos ahora como `autograd` hace nuestra red neuronal mas simple.\n\n## Red Neuronal, usando autograd\n\nGracias a `autograd` podemos despedirnos del proceso tedioso y propenso a errores de programar la retro-propagación por nosotros mismos. Un único método hace todo esto: `loss$backward()`.\n\nCon `torch` registrando las operaciones, no se requiere nombrar explícitamente los tensores intermedios. Podemos programar la propagación hacia adelante, hacer el calculo de las perdidas y la retro-propagación en solo tres lineas:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\nloss <- (y_pred - y)$pow(2)$sum()\nloss$backward\n```\n:::\n\n\nA continuación el código completo. Estamos en una fase intermedia: Aun tenemos que calcular manualmente la propagación hacia adelante y las perdidas y aun tenemos que actualizar manualmente los pesos. Por esto último, hay algo que requiere ser explicado, pero revisemos primero la nueva versión de la red neuronal:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\n### Generación de los datos de entrenamiento\n\n# Dimensión de la entrada (número de características de entrada)\nd_in <- 3\n# Dimensión de la salida (número de características predecidas)\nd_out <- 1\n# Número de observaciones del conjunto de entrenamiento\nn <- 100\n\n# Creación de datos aleatorios\nx <- torch_randn(n, d_in)\ny <- x[, 1, drop = F] * 0.2 - x[, 2, drop = F] *1.3 - x[, 3, drop = F] * 0.5 + torch_randn(n, 1)\n\n### inicialización de los pesos\n\n# Dimensiones de la capa oculta\nd_hidden <- 32\n# Pesos que conectan la entrada con la capa oculta\nw1 <- torch_randn(d_in, d_hidden, requires_grad = T)\n# Pesos que conectan la capa oculta con la salida\nw2 <- torch_randn(d_hidden, d_out, requires_grad = T)\n\n# Sesgos de la capa oculta\nb1 <- torch_zeros(1, d_hidden, requires_grad = T)\n# Sesgos de la capa de salida\nb2 <- torch_zeros(1, d_out, requires_grad = T)\n\n### Parámetros de la red\n\nlearning_rate <- 1e-4\n\n### Ciclo de entrenamiento\n\nfor(t in 1:200){\n  ### Propagación hacia adelante\n  y_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\n  # clamp simula \n  \n  ### Cálculo de la perdida\n  loss <- (y_pred-y)$pow(2)$sum()\n  if(t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### Retro-propagación\n  \n  # cálculo del gradiente, todos los tensores con requires_grad = TRUE\n  loss$backward()\n  \n  ### Actualización de los pesos\n  \n  # se ejecuta con with_no_grad() porque esta parte no queremos que se \n  # haga calculo automático del gradiente\n  with_no_grad({\n    w1 <- w1$sub_(learning_rate * w1$grad)\n    w2 <- w2$sub_(learning_rate * w2$grad)\n    b1 <- b1$sub_(learning_rate * b1$grad)\n    b2 <- b2$sub_(learning_rate * b2$grad)\n    # el método sub_ parece que resta al tensor original el argumento\n    \n    # Actualización de los gradientes después de cada ciclo, de \n    # otro modo se acumularían\n    w1$grad$zero_()\n    w2$grad$zero_()\n    b1$grad$zero_()\n    b2$grad$zero_()\n  })\n  \n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  10    Loss:  176.326 \nEpoch:  20    Loss:  126.9562 \nEpoch:  30    Loss:  115.167 \nEpoch:  40    Loss:  110.2214 \nEpoch:  50    Loss:  107.4412 \nEpoch:  60    Loss:  104.9799 \nEpoch:  70    Loss:  102.9808 \nEpoch:  80    Loss:  101.2993 \nEpoch:  90    Loss:  99.50574 \nEpoch:  100    Loss:  97.84877 \nEpoch:  110    Loss:  96.35872 \nEpoch:  120    Loss:  95.11468 \nEpoch:  130    Loss:  94.0111 \nEpoch:  140    Loss:  92.97823 \nEpoch:  150    Loss:  91.99876 \nEpoch:  160    Loss:  90.98146 \nEpoch:  170    Loss:  90.04476 \nEpoch:  180    Loss:  89.19314 \nEpoch:  190    Loss:  88.43552 \nEpoch:  200    Loss:  87.65956 \n```\n:::\n:::\n\n\nComo se explicó antes, después de `algun_tensor$backward()`, todos los tensores precedentes en el grafo actualizarán los campos `grad`[^2]. Nosotros hacemos uso de esos campos para actualizar los pesos, pero ahora `autograd` esta \"encendido\", siempre que se ejecute una operación esta quedará registrada para la retro-propagación, por lo cual se requiere desactivar el `autograd`de forma explicita usando `with_no_grad()`.\n\n[^2]: Para ser mas precisos, solo se requiere que `requires_grad` este configurado en `TRUE`.\n\nMientras que esto es algo que podríamos clasificar como \"bueno saberlo\", dado que una vez lleguemos al último documento de la serie, la actualización de los pesos también será automatizada, el concepto de reiniciar en cero los gradientes esta aquí para quedarse: los valores almacenados en los campos `grad`se acumulan, en cualquier parte donde se usen, será necesario reiniciarlos en cero para reutilizarlos.\n\n### En resumen\n\nEn donde quedamos? Se inicio con la programación de la red neuronal completamente de cero, únicamente usando tensores. Ahora obtuvimos una ayuda significativa usando `autograd`.\n\nPero aun estamos usando la actualización manual de los pesos y aun no tenemos abstracciones que nos permitan un fácil desarrollo de arquitecturas de aprendizaje profundo (\"Capas\" o \"Módulos\").\n\nAmbos temas serán tratados en las próximas entregas. Gracias por leernos!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}