{
  "hash": "618179a27454bd4f003be466beb23db1",
  "result": {
    "markdown": "---\ntitle: \"Usando módulos de torch\"\nauthor: \"Jhonatan Zambrano\"\ndate: \"2023-01-17\"\ncategories: [autograd, torch, backprop]\nabstract: \"En esta tercera entrega de la mini serie de bases de torch, reemplazamos las operaciones con matrices programadas manualmente por modulos, simplificando considerablemente el codigo de prueba de nuestra red.\"\nimage: \"image.jpg\"\nlang: \"es\"\ndraft: true\n---\n\n\nInicialmente, empezamos aprendiendo sobre las bases de torch programando una red neuronal sencilla \"desde el principio\", haciendo uso solamente de una de las características de torch: los tensores. Ahora, vamos a simplificar muchísimo la tarea reemplazando la propagación hacia atrás manual con el \"autograd\". Hoy vamos a modularizar la red en dos sentidos: en el habitual y en uno mucho mas literal: las operaciones de matrices de bajo nivel son reemplazadas por módulos de torch.\n\n## Módulos\n\nEn otras plataformas (por ejemplo keras), se puede estar acostumbrado a distinguir entre módulos y capas. En torch, ambos son instancias de `nn_module()`, y por lo tanto, se tienen algunos métodos en común. Para aquellos que piensan en términos de \"modelos\" y \"capas\", se dividió artificialmente esta sección en dos partes. En realidad no hay dicotomia: nuevos módulos pueden estar compuestos de otros existentes en niveles arbitrarios de recursión.\n\n### Modulos Base (Capas)\n\nEn lugar de escribir una transformación afin manualmente: `x$mm(w1) + b1` por ejemplo, como lo hemos hecho hasta ahora, podemos crear un modulo lineal. El siguiente fragmento de código crea una instancia de una capa lineal que espera como entrada tres variables y devuelve una salida por observación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nl <- nn_linear(3, 1)\n```\n:::\n\n\nEl módulo tiene dos parámetros, \"Peso\" y \"Sesgo\". Ambos vienen pre-inicializados:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl$parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.5032 -0.3110 -0.4619\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n0.01 *\n 1.9172\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nLos módulos pueden ser invocables; la invocación de un módulo ejecuta el método `forward()`, el cual, para una capa lineal, multiplica (matricialmente) la entrada por los pesos y suma el sesgo.\n\nIntentemos lo siguiente:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- torch_randn(10, 3)\nout <- l(data)\n```\n:::\n\n\nComo se esperaba, la salida (out) ahora contiene algunos datos:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n-0.0480\n 0.5198\n 0.6608\n-0.0251\n-0.7116\n 0.0869\n 1.0596\n-0.0275\n-0.7691\n 0.4983\n[ CPUFloatType{10,1} ]\n```\n:::\n:::\n\n\nAdicionalmente, este tensor sabe que requiere ser hecho siempre que se le pida calcular gradientes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$grad_fn\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAddmmBackward0\n```\n:::\n:::\n\n\nNótese la diferencia entre los tensores retornados por los módulos y los creados por comandos nuestros. Cuando creamos los tensores, se requiere definir `requires_grad = TRUE` para activar el calculo de gradientes. Con los módulos, torch asume correctamente que deseamos realizar la propagacion hacia atras en algun momento.\n\nPor ahora, no hemos invocado `backward()` aun. Asi que aun no se han calculado ningun gradiente:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl$weight$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n[ Tensor (undefined) ]\n```\n:::\n\n```{.r .cell-code}\nl$bias$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n[ Tensor (undefined) ]\n```\n:::\n:::\n\n\nCambiemos esto:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$backward()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in (function (self, inputs, gradient, retain_graph, create_graph) : grad can be implicitly created only for scalar outputs\nException raised from _make_grads at ../torch/csrc/autograd/autograd.cpp:47 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f09efe452eb in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0xd1 (0x7f09efe40e41 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #2: <unknown function> + 0x38f064f (0x7f09c52f064f in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #3: torch::autograd::backward(std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, c10::optional<bool>, bool, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) + 0x45 (0x7f09c52f2035 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #4: <unknown function> + 0x395ccfe (0x7f09c535ccfe in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #5: at::Tensor::_backward(c10::ArrayRef<at::Tensor>, c10::optional<at::Tensor> const&, c10::optional<bool>, bool) const + 0x49 (0x7f09c2cfc8d9 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #6: _lantern_Tensor__backward_tensor_tensorlist_tensor_bool_bool + 0x17b (0x7f09f074d035 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/liblantern.so)\nframe #7: <unknown function> + 0x594af5 (0x7f09f4194af5 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #8: std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, std::__future_base::_Task_state<std::function<void ()>, std::allocator<int>, void ()>::_M_run()::{lambda()#1}, void> >::_M_invoke(std::_Any_data const&) + 0x35 (0x7f09f41968e5 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #9: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*) + 0x2d (0x7f09f4196c1d in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #10: <unknown function> + 0x99f68 (0x7f09ff099f68 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #11: EventLoop<void>::run() + 0x1a4 (0x7f09f4198964 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #12: <unknown function> + 0xdc2b3 (0x7f09fc8dc2b3 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #13: <unknown function> + 0x94b43 (0x7f09ff094b43 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #14: <unknown function> + 0x126a00 (0x7f09ff126a00 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n```\n:::\n:::\n\n\n¿Porque se produce un error? *Autograd* espera que el tensor de salida sea un escalar, mientras que en nuestro ejemplo tenemos un tensor de tamaño (10, 1). Este error no ocurriría en la practica, donde se trabaja por baches de entradas (en ocasiones, solo un único bache). Pero aun así, es interesante ver como resolver esto.\n\nPara hacer que nuestro ejemplo funcione, se introduce un paso adicional, calculo de una media virtual. LLamemoslo `avg`. Si tal media fuera calculada, su gradiente con respecto a `l$weight` podría ser obtenida vía \"regla de la cadena\":\n\n$$ \n\\frac{\\partial avg}{\\partial w} = \\frac{\\partial avg}{\\partial out}\n\\frac{\\partial out}{\\partial w}\n$$\n\nDe las cantidades de la derecha, estamos interesados en la segunda. Se necesita proveer la primera, de la forma en que esto podría verse *si realmente estuviéramos calculando la media*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_avg_d_out <- torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t()\nout$backward(gradient = d_avg_d_out)\n```\n:::\n\n\nAhora, `l$wieght$grad` y `l$bias$grad` si contienen los gradientes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl$weight$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 25.0574   4.6675   1.3768\n[ CPUFloatType{1,3} ]\n```\n:::\n\n```{.r .cell-code}\nl$bias$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 100\n[ CPUFloatType{1} ]\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}