{
  "hash": "0a801c244fab8ed1b7a60cea11ac947a",
  "result": {
    "markdown": "---\ntitle: \"Usando módulos de torch\"\nauthor: \"Jhonatan Zambrano\"\ndate: \"2023-03-06\"\ncategories: [torch, modulos, capas]\nabstract: \"En esta tercera entrega de la mini serie de bases de torch, reemplazamos las operaciones con matrices programadas manualmente por modulos, simplificando considerablemente el codigo de prueba de nuestra red.\"\nimage: \"tetris.png\"\nlang: \"es\"\ndraft: false\n---\n\n\n## Introducción\n\n![](tetris.png)\n\nInicialmente, empezamos aprendiendo sobre las bases de `torch` programando una red neuronal sencilla \"desde el principio\", haciendo uso solamente de una de las características de `torch`: los tensores. Ahora, vamos a simplificar muchísimo la tarea reemplazando la retro-propagación con `autograd`. Hoy vamos a modularizar la red en dos sentidos: en el habitual y en uno mucho mas literal: las operaciones de matrices de bajo nivel son reemplazadas por módulos de `torch`.\n\n## Módulos\n\nEn otras plataformas (por ejemplo `keras`), se puede estar acostumbrado a distinguir entre módulos y capas. En `torch`, ambos son instancias de `nn_module()`, y por lo tanto, se tienen algunos métodos en común. Para aquellos que piensan en términos de \"modelos\" y \"capas\", se dividió artificialmente esta sección en dos partes. En realidad no hay dicotomía: nuevos módulos pueden estar compuestos de módulos existentes en niveles arbitrarios de recursión.\n\n### Modulos Base (Capas)\n\nEn lugar de escribir una transformación afin manualmente: `x$mm(w1) + b1` por ejemplo, como lo hemos hecho hasta ahora, podemos crear un modulo lineal. El siguiente fragmento de código crea una instancia de una capa lineal que espera como entrada tres variables y devuelve una salida por observación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nl <- nn_linear(3, 1)\n```\n:::\n\n\nEl módulo tiene dos parámetros, \"Peso\" y \"Sesgo\". Ambos vienen pre-inicializados:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl$parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$weight\ntorch_tensor\n 0.1647  0.5118 -0.1313\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n 0.4457\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nLos módulos pueden ser invocados; la invocación de un módulo ejecuta el método `forward()`, el cual, para una capa lineal, multiplica (matricialmente) la entrada por los pesos y suma el sesgo.\n\nIntentemos lo siguiente:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- torch_randn(10, 3)\nout <- l(data)\n```\n:::\n\n\nComo se esperaba, la salida `out` ahora contiene algunos datos:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 0.8072\n 0.1507\n 0.9358\n 0.2930\n 0.7551\n 0.1299\n-0.4496\n-0.2296\n-0.2524\n 0.5204\n[ CPUFloatType{10,1} ]\n```\n:::\n:::\n\n\nAdicionalmente, este tensor sabe que requiere ser hecho siempre que se le pida calcular gradientes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$grad_fn\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAddmmBackward0\n```\n:::\n:::\n\n\nNótese la diferencia entre los tensores retornados por los módulos y los creados por comandos nuestros. Cuando creamos los tensores, se requiere definir `requires_grad = TRUE` para activar el calculo de gradientes. Con los módulos, `torch` asume correctamente que deseamos realizar la retro-propagación en algún momento.\n\nPor ahora, no hemos invocado `backward()` aun. Así que aún no se ha calculado ningún gradiente:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl$weight$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n[ Tensor (undefined) ]\n```\n:::\n\n```{.r .cell-code}\nl$bias$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n[ Tensor (undefined) ]\n```\n:::\n:::\n\n\nCambiemos esto:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$backward()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in (function (self, inputs, gradient, retain_graph, create_graph) : grad can be implicitly created only for scalar outputs\nException raised from _make_grads at ../torch/csrc/autograd/autograd.cpp:47 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7fd6b34452eb in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0xd1 (0x7fd6b3440e41 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #2: <unknown function> + 0x38f064f (0x7fd6888f064f in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #3: torch::autograd::backward(std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, c10::optional<bool>, bool, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) + 0x45 (0x7fd6888f2035 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #4: <unknown function> + 0x395ccfe (0x7fd68895ccfe in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #5: at::Tensor::_backward(c10::ArrayRef<at::Tensor>, c10::optional<at::Tensor> const&, c10::optional<bool>, bool) const + 0x49 (0x7fd6862fc8d9 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #6: _lantern_Tensor__backward_tensor_tensorlist_tensor_bool_bool + 0x17b (0x7fd6b3d4d035 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/liblantern.so)\nframe #7: <unknown function> + 0x5947c5 (0x7fd6b77947c5 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #8: std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, std::__future_base::_Task_state<std::function<void ()>, std::allocator<int>, void ()>::_M_run()::{lambda()#1}, void> >::_M_invoke(std::_Any_data const&) + 0x35 (0x7fd6b77965b5 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #9: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*) + 0x2d (0x7fd6b77968ed in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #10: <unknown function> + 0x99f68 (0x7fd6c2899f68 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #11: EventLoop<void>::run() + 0x1a4 (0x7fd6b7798634 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #12: <unknown function> + 0xdc2b3 (0x7fd6bfedc2b3 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #13: <unknown function> + 0x94b43 (0x7fd6c2894b43 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #14: <unknown function> + 0x126a00 (0x7fd6c2926a00 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n```\n:::\n:::\n\n\n¿Porque se produce un error? `autograd` espera que el tensor de salida sea un escalar, mientras que en nuestro ejemplo tenemos un tensor de tamaño (10, 1). Este error no ocurriría en la practica, donde se trabaja por baches de entradas (en ocasiones, solo un único bache). Pero aun así, es interesante ver como resolver esto.\n\nPara hacer que nuestro ejemplo funcione, se introduce un paso adicional, calculo de una media virtual. LLamemoslo `avg`. Si tal media fuera calculada, su gradiente con respecto a `l$weight` podría ser obtenida vía *regla de la cadena*:\n\n$$ \n\\frac{\\partial avg}{\\partial w} = \\frac{\\partial avg}{\\partial out}\n\\frac{\\partial out}{\\partial w}\n$$\n\nDe las cantidades de la derecha, estamos interesados en la segunda. Se necesita proveer la primera, de la forma en que esto podría verse *si realmente estuviéramos calculando la media*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_avg_d_out <- torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t()\nout$backward(gradient = d_avg_d_out)\n```\n:::\n\n\nAhora, `l$wieght$grad` y `l$bias$grad` sí contienen los gradientes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl$weight$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n-27.2703 -23.9543   9.2242\n[ CPUFloatType{1,3} ]\n```\n:::\n\n```{.r .cell-code}\nl$bias$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 100\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nAdicionalmente a `nn_linear()`, `torch` provee mucho de todo los que se espera de las capas mas comunes. Aún así, algunas tareas se resuelven por una sola capa ¿como combinarlas? o, en lenguaje común: ¿como construir modelos?\n\n### Módulos contenedores (\"Modelos\")\n\nBien, los *modelos* son módulos que contienen otros módulos. Por ejemplo, si todas las entradas se supone que fluyen atraves de los mismos nodos y a lo largo de las mismas vías, entonces `nn_sequentiual()` puede usarse para construir un grafo sencillo.\n\nPor ejemplo:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_sequential(\n  nn_linear(3, 16),\n  nn_relu(),\n  nn_linear(16, 1)\n)\n```\n:::\n\n\npodemos usar la misma técnica de arriba para ver los parámetros del modelo (dos matrices de pesos y dos vectores de sesgo):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`0.weight`\ntorch_tensor\n 0.5355 -0.0213  0.4647\n 0.1244 -0.4988 -0.4234\n 0.2565  0.2977  0.2528\n 0.5149 -0.0701 -0.0679\n-0.4810  0.1394 -0.2943\n-0.4708  0.4047 -0.5536\n 0.0525 -0.4987  0.3909\n-0.4642  0.2297 -0.1231\n-0.4173 -0.2474 -0.4889\n-0.2684 -0.4513  0.2920\n 0.3599 -0.5580 -0.0656\n-0.5073  0.0704  0.4467\n 0.1711  0.5273  0.2958\n-0.0286 -0.4735 -0.1189\n 0.0977  0.2855 -0.2965\n 0.0773 -0.3401  0.5104\n[ CPUFloatType{16,3} ][ requires_grad = TRUE ]\n\n$`0.bias`\ntorch_tensor\n 0.1984\n 0.5505\n-0.4983\n 0.3101\n-0.3564\n 0.0365\n-0.4945\n 0.3637\n 0.1587\n 0.5755\n 0.1083\n 0.2867\n 0.0973\n-0.2592\n 0.1121\n-0.0347\n[ CPUFloatType{16} ][ requires_grad = TRUE ]\n\n$`2.weight`\ntorch_tensor\nColumns 1 to 10-0.2272 -0.0307  0.1232 -0.1382 -0.0439  0.1983  0.1353  0.0458 -0.2376  0.2480\n\nColumns 11 to 16 0.2444 -0.1623 -0.0844 -0.0362 -0.1659 -0.0595\n[ CPUFloatType{1,16} ][ requires_grad = TRUE ]\n\n$`2.bias`\ntorch_tensor\n 0.1119\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nPara inspeccionar un parámetro individual, hay que usar la posición en el modelo secuencial. Por ejemplo:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel[[1]]$bias\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 0.1984\n 0.5505\n-0.4983\n 0.3101\n-0.3564\n 0.0365\n-0.4945\n 0.3637\n 0.1587\n 0.5755\n 0.1083\n 0.2867\n 0.0973\n-0.2592\n 0.1121\n-0.0347\n[ CPUFloatType{16} ][ requires_grad = TRUE ]\n```\n:::\n:::\n\n\nY del mismo modo que antes en `nn_linear()`, este módulo puede ser invocado directamente sobre los datos:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- model(data)\n```\n:::\n\n\nEn un módulo compuesto (modelo) como este, invocar `backward()` hará la retro-propagación a través de todas las capas:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t())\n\nmodel[[1]]$bias$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n-13.6346\n -2.7662\n  0.0000\n -6.9083\n -1.3165\n  5.9494\n  4.0576\n  3.2053\n-16.6351\n 22.3234\n 14.6633\n -9.7406\n -3.3776\n -1.4462\n -9.9561\n -4.1668\n[ CPUFloatType{16} ]\n```\n:::\n:::\n\n\nY *ubicando* el módulo compuesto (modelo) en la GPU moverá todos los tensores allí:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$cuda()\nmodel[[1]]$bias$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n-13.6346\n -2.7662\n  0.0000\n -6.9083\n -1.3165\n  5.9494\n  4.0576\n  3.2053\n-16.6351\n 22.3234\n 14.6633\n -9.7406\n -3.3776\n -1.4462\n -9.9561\n -4.1668\n[ CUDAFloatType{16} ]\n```\n:::\n:::\n\n\nVeamos ahora cómo, usando `nn_sequential()` se puede simplificar nuestra red neuronal de ejemplo.\n\n## Red neuronal simple usando módulos\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### generación de datos de entrenamiento ---------------\n\n# dimensiones de la entrada (número de características de entrada)\nd_in <- 3\n# dimensiones de la salida (número de características de predicción)\nd_out <- 1\n# número de observaciones en el conjunto de entrenamiento\nn <- 100\n\n# Creación de datos aleatorios\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n# Nótese que se usan NULL, pero podría reemplazarse por el parámetro drop = FALSE, sirve para asegurarse que no se pierde las dimensiones originales de los tensores al hacer la selección\n\n### Definición de la red neuronal\n\n# dimensiones de la capa oculta\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### Parámetros de la red\n\nlearning_rate <- 1e-4\n\n### Ciclo de entrenamiento\n\nfor (t in 1:200){\n  ### ------ propagación hacia adelante---------\n  \n  y_pred <- model(x)\n  \n  ### ------ cálculo de perdidas --------\n  \n  loss <- (y_pred - y)$pow(2)$sum()\n  if(t %% 10 ==0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### ------- retro-propagación ---------\n  \n  # puesta a cero de los gradientes antes de iniciar la retro-propagación\n  model$zero_grad()\n  \n  # Cálculo de los gradientes para los parámetros del modelo\n  loss$backward()\n  \n  ### ------- actualizacion de los pesos -------\n  # se ejecuta con with_no_grad() porque en esta parte no se desea almacenar el calculo\n  # automático del gradiente\n  # Se actualiza cada parámetro con su respectivo `grad`\n  \n  with_no_grad({\n    model$parameters %>% purrr::walk(function(param) param$sub_(learning_rate * param$grad))\n  })\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  10    Loss:  248.7436 \nEpoch:  20    Loss:  178.2965 \nEpoch:  30    Loss:  141.0255 \nEpoch:  40    Loss:  122.7597 \nEpoch:  50    Loss:  114.196 \nEpoch:  60    Loss:  110.0747 \nEpoch:  70    Loss:  107.8717 \nEpoch:  80    Loss:  106.5085 \nEpoch:  90    Loss:  105.5374 \nEpoch:  100    Loss:  104.7832 \nEpoch:  110    Loss:  104.1391 \nEpoch:  120    Loss:  103.556 \nEpoch:  130    Loss:  102.988 \nEpoch:  140    Loss:  102.452 \nEpoch:  150    Loss:  101.9532 \nEpoch:  160    Loss:  101.4878 \nEpoch:  170    Loss:  101.0473 \nEpoch:  180    Loss:  100.6296 \nEpoch:  190    Loss:  100.2312 \nEpoch:  200    Loss:  99.85239 \n```\n:::\n:::\n\n\nLa propagación hacia adelante se ve mucho mas simple ahora; sin embargo, aun tenemos que hacer el ciclo sobre los parámetros del modelo y la actualización de cada uno manualmente. Posiblemente, usted puede sospechar que `torch` provee abstracciones para funciones comunes de funciones de perdidas. En la próxima entrega de esta serie (que ademas será la final), vamos a tratar estos dos puntos, haciendo uso de las perdidas y optimizadores de `torch`. Nos veremos entonces!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}