[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Torch para R",
    "section": "",
    "text": "Optimizadores en torch\n\n\n\n\n\n\n\ntorch\n\n\noptimizadores\n\n\n\n\nAqui concluye la mini-serie de bases de torch, adicionando a nuestra caja de herramientas dos abstracciones: funciones de perdidas y optimizadores\n\n\n\n\n\n\nMar 16, 2023\n\n\nJhonatan Zambrano\n\n\n\n\n\n\n  \n\n\n\n\nUsando módulos de torch\n\n\n\n\n\n\n\ntorch\n\n\nmodulos\n\n\ncapas\n\n\n\n\nEn esta tercera entrega de la mini serie de bases de torch, reemplazamos las operaciones con matrices programadas manualmente por modulos, simplificando considerablemente el codigo de prueba de nuestra red.\n\n\n\n\n\n\nMar 6, 2023\n\n\nJhonatan Zambrano\n\n\n\n\n\n\n  \n\n\n\n\nPresentando el autograd de torch\n\n\n\n\n\n\n\ntorch\n\n\nautograd\n\n\n\n\nCon torch, es dificil encontrar una razon para programar desde el principio la retro-propagación. Su caracteristica de diferenciación automática, llamada autograd, lleva un registro de las operaciones que sus computos del gradiente requiere, asi como tambien el cómo calcularlos. En esta segunda parte de una serie de cuatro, actualizamos nuestra red neuronal simple de la primera entrega para hacer uso de autograd.\n\n\n\n\n\n\nFeb 14, 2023\n\n\nJhonatan Zambrano\n\n\n\n\n\n\n  \n\n\n\n\nFamiliarizandose con tensores en torch\n\n\n\n\n\n\n\ntorch\n\n\ntensores\n\n\n\n\nSe presentan aquí las cosas principales que necesitas saber sobre los tensores de Torch. Como ejemplo ilustrativo se va a programar una red neuronal sencilla desde el principio.\n\n\n\n\n\n\nJan 17, 2023\n\n\nJhonatan Zambrano\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Este blog tiene como propósito principalmente la traducción al español del blog de la compañía posit “AI Blog” para las entradas referentes a “torch for R”. Las traducciones tratan de ser lo mas fiables posibles a los originales en inglés en cuanto a las ideas presentadas, lo cual, en ocasiones, puede llevar a ligeras diferencias entre lo escrito textualmente en inglés original y mi traducción al español.\nEsta no es una traducción oficial, ni ha sido revisada por nadie mas, si se tienen recomendaciones, o sugerencias, los aportes son bienvenidos en el código que se sube al GitHub.\nEventualmente, haré entregas propias, de mis trabajos desarrollados en torch, serán identificados porque dentro de las categorías del articulo no aparecerá “torch”."
  },
  {
    "objectID": "posts/Tensores/index.html#introducción",
    "href": "posts/Tensores/index.html#introducción",
    "title": "Familiarizandose con tensores en torch",
    "section": "Introducción",
    "text": "Introducción\nAnteriormente se introdujo torch, un paquete de R que provee funcionalidad nativa similar a la que tienen los usuarios de Python por medio de PyTorch. Allí se asumió algún conocimiento de Keras y TensorFlow. Por lo anterior, se “retrató” torch de forma que pudiera ser de ayuda para alguien que haya “crecido” con la forma en que se entrena un modelo en Keras : enfocándose en las diferencias, sin perder de vista el proceso completo.\nEn esta publicación se cambia de perspectiva. Se programa una red neuronal sencilla “desde el principio” haciendo uso únicamente de uno de los bloques constructivos básicos de torch: tensores. Esta red sera tan de “bajo nivel” como puede es posible. (Para aquellos menos inclinados a las matemáticas, esto puede servir como un repaso sobre que es lo que ocurre realmente detrás de todas las herramientas que han sido convenientemente construidas para nosotros. Pero el propósito real es ilustrar todo lo que se puede hacer únicamente con tensores).\nPosteriormente, se publicaran tres documentos que mostrarán progresivamente como se puede ir reduciendo el esfuerzo: notablemente desde el principio, enormemente una vez se hayan terminado. Al finalizar estas publicaciones habrás visto como la derivación automática funciona en torch, como usar módulos (capas, in el idioma de keras) y optimizadores. Para ese entonces, tendrás fuertes bases para ser usadas cuando se aplique torch al desarrollo de tareas del mundo real.\nEsta publicación será la mas extensa, dado que hay mucho por aprender acerca de los tensores: como crearlos, como manipular sus contenidos o modificas sus formas, como convertirlos en arreglos de R, matrices o vectores, y por supuesto, dada la omnipresente necesidad de velocidad, como ejecutar todas estas operaciones en la GPU. Una vez cumplida la agenda, programaremos la mencionada red neuronal, observando todos estos aspectos en acción."
  },
  {
    "objectID": "posts/Tensores/index.html#tensores",
    "href": "posts/Tensores/index.html#tensores",
    "title": "Familiarizandose con tensores en torch",
    "section": "Tensores",
    "text": "Tensores\n\nCreación\nLos tensores pueden ser creados especificando los valores individuales. aqui se crean dos tensores uni-dimensionales (vectores), de tipo “float” y “bool” respectivamente:\n\nlibrary(torch)\n# un vector 1d de tamaño 2\nt <- torch_tensor(c(1, 2))\nt\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\n# Ahora un vector 1d, pero del tipo bool\nt<-torch_tensor(c(TRUE, FALSE))\n\nY aquí se presentan dos modos de crear tensores bi-dimensionales (matrices). Note como en el segundo modo se necesita especificar byrow = TRUE en el llamado a matrix()para obtener los valores ordenados en orden fila-mayor.\n\n# un tensor de 3x3 (matriz)\nt <- torch_tensor(rbind(c(1, 2, 0), c(3, 0, 0), c(4, 5, 6)))\nt\n\ntorch_tensor\n 1  2  0\n 3  0  0\n 4  5  6\n[ CPUFloatType{3,3} ]\n\n# otro tensor de 3x3\nt <- torch_tensor(matrix(1:9, ncol = 3, byrow = T))\nt\n\ntorch_tensor\n 1  2  3\n 4  5  6\n 7  8  9\n[ CPULongType{3,3} ]\n\n\nPara dimensiones mas altas especialmente, puede se mas facil especificar el tipo de tensor de forma abstracta, como en: “dame un tensor de <…> de la forma n1 x n2” donde <…> puede ser “ceros”, “unos”, o por ejemplo, “valores muestreados de una distribución normal estándar”:\n\n# un tensor de 3x3 de valores normalmente distribuidos\nt <- torch_randn(3, 3)\nt\n\ntorch_tensor\n 1.6203 -0.7829 -0.7159\n 1.3453 -1.6175 -0.3454\n-1.6268 -0.2089 -0.2962\n[ CPUFloatType{3,3} ]\n\n# un tensor de ceros de 4x2x2 (3d)\nt <- torch_zeros(4, 2, 2)\nt\n\ntorch_tensor\n(1,.,.) = \n  0  0\n  0  0\n\n(2,.,.) = \n  0  0\n  0  0\n\n(3,.,.) = \n  0  0\n  0  0\n\n(4,.,.) = \n  0  0\n  0  0\n[ CPUFloatType{4,2,2} ]\n\n\nExisten muchas funciones similares, incluidas: torch_arange() para crear un tensor que mantiene una secuencia de valores igualmente espaciados, torch_eye() el cual retorna una matriz identidad y torch_logspace() que llena un rango especifico con una lista de valores espaciados logarítmicamente.\nSi el argumento dtype no se especifica, torch inferirá el tipo de datos de los valores entregados. Por ejemplo:\n\nt <- torch_tensor(c(3, 5, 7))\nt$dtype\n\ntorch_Float\n\nt <- torch_tensor(1L)\nt$dtype\n\ntorch_Long\n\n\nPero se puede definir explícitamente un dtype diferente si se desea:\n\nt <- torch_tensor(1, dtype = torch_double())\nt$dtype\n\ntorch_Double\n\n\nLos tensores de torch residen en un dispositivo. Por defecto, será en la CPU:\n\nt$device\n\ntorch_device(type='cpu')\n\n\nAunque se puede definir un tensor que resida en la GPU:\n\nt <- torch_tensor(2, device = \"cuda\")\nt$device\n\ntorch_device(type='cuda', index=0)\n\n\nSe hablará mas sobre los dispositivos mas adelante.\nHay otro parámetro importante en las funciones para creación de tensores: requires_grad. Sin embargo, aquí debemos apelar a la paciencia, este tema sera discutido de forma prominente en la siguiente publicación.\n\n\nConversión a tipos de datos nativos de R\nPara convertir tensores torch a datos nativos de R se usa la función as_array():\n\nt <- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nas.array(t)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nDependiente de si el tensor es de una, dos o tres dimensiones, el objeto resultante nativo será un vector, una matriz o un arreglo:\n\nt <- torch_tensor(c(1, 2, 3))\nas.array(t) %>% class()\n\n[1] \"numeric\"\n\nt <- torch_ones(c(2, 2))\nas.array(t) %>% class()\n\n[1] \"matrix\" \"array\" \n\nt <- torch_ones(c(2, 2, 2))\nas.array(t) %>% class()\n\n[1] \"array\"\n\n\nPara tensores de una o dos dimensiones, también es posible usar as.integer() o as.matrix().\nSi un tensor actualmente reside en la GPU, se requiere moverlo a la CPU primero:\n\nt <- torch_tensor(2, device = \"cuda\")\nas.integer(t$cpu())\n\n[1] 2\n\n\n\n\nIndexado y seccionado de tensores\nA menudo se desea obtener solo una parte de un tensor, incluso un único valor. En estos casos se habla de seccionado e indexado respectivamente.\nEn R, estas operaciones son base-1 es decir, la primera posición de cualquier arreglo se identifica con el número 1 y no con el número 0. El mismo comportamiento fue implementado para torch. De este modo, muchas de la funcionalidad descrita en esta sección se podría sentir intuitiva.\n\n\nLa parte similar a R\nNada de lo siguiente debería parecer demasiado sorpresivo:\n\nt <- torch_tensor(rbind(c(1, 2, 3), c(4, 5, 6)))\nt\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPUFloatType{2,3} ]\n\n# Un unico valor\nt[1, 1]\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n# primera fila, todas las columnas\nt[1, ]\n\ntorch_tensor\n 1\n 2\n 3\n[ CPUFloatType{3} ]\n\n# primera fila, un subconjunto de columnas\nt[1, 1:2]\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\n\nNótese como, tal y como ocurre en R, las dimensiones son eliminadas\n\nt <- torch_tensor(rbind(c(1, 2, 3), c(4, 5, 6)))\n\n# 2x3\nt$size()\n\n[1] 2 3\n\n# Una sola fila: sera devuelto como un vector\nt[1, 1:2]$size()\n\n[1] 2\n\n# Un solo elemento\nt[1, 1]$size()\n\ninteger(0)\n\n\nY al igual que en R, se pueden mantener las dimensiones originales si se especifica drop = FALSE:\n\nt[1, 1:2, drop = F]$size()\n\n[1] 1 2\n\nt[1, 1, drop = F]$size()\n\n[1] 1 1\n\n\n\n\nLa parte distinta a R\nR usa números negativos para remover elementos en posiciones especificas, en torch los números negativos indican que se inicia contando desde el final de un tensor, siendo -1 el último elemento:\n\nt <- torch_tensor(rbind(c(1, 2, 3), c(4, 5, 6)))\n\nt[1, -1]\n\ntorch_tensor\n3\n[ CPUFloatType{} ]\n\nt[ , -2:-1]\n\ntorch_tensor\n 2  3\n 5  6\n[ CPUFloatType{2,2} ]\n\n\nEsta característica puede ser conocida de NumPy. Al igual que la siguiente:\nCuando la expresión de rebanado m:n se aumenta con un tercer numero m:n:o se tomará cada o-ésimo ítem del rango especificado por m y n:\n\nt <- torch_tensor(1:10)\nt[2:10:2]\n\ntorch_tensor\n  2\n  4\n  6\n  8\n 10\n[ CPULongType{5} ]\n\n\nAlgunas veces no se sabe cuantas dimensiones tiene un tensor, pero sí sabemos que hacer con la última dimensión, o la primera. Para obviar todas las otras podemos usar:\n\nt <- torch_randint(-7, 7, size = c(2, 2, 2))\nt\n\ntorch_tensor\n(1,.,.) = \n  5  5\n  1 -1\n\n(2,.,.) = \n -1 -2\n  4  5\n[ CPUFloatType{2,2,2} ]\n\nt[.., 1]\n\ntorch_tensor\n 5  1\n-1  4\n[ CPUFloatType{2,2} ]\n\nt[2, ..]\n\ntorch_tensor\n-1 -2\n 4  5\n[ CPUFloatType{2,2} ]\n\n\nPasamos ahora a un tema que, en la practica, es tan indispensable como el seccionamiento: cambios en la forma de los tensores.\n\n\nCambiando la forma de los tensores\nLoa cambios en las formas de los tensores pueden ocurrir de dos formas fundamentalmente. Observando lo que el “reformado” es realmente: mantener los valores pero modifica el arreglo, podríamos ya sea, alterar como los valores están distribuidos físicamente, o mantener a estructura física como está y solo cambiar el “mapeo”, es decir, un cambio semántico.\nEn el primer caso, se debe apartar almacenamiento para dos tensores, la fuente y el destino, los elementos serán copiados del último al primero. En el segundo caso, físicamente solo habrá un tensor, referenciado por dos entidades lógicas con distintos metadatos.\nNo es de sorprenderse que por razones de rendimiento, sean preferidas las operaciones del segundo caso.\n\nReformado copia cero\nEmpezamos con métodos de copia cero, dado que serán usados siempre que podamos.\nUn caso especial a menudo visto en la practica es adicionar o remover dimensiones con un solo elemento.\nunsqueeze() adiciona una dimensión de tamaño 1 a la posición especificada por dim:\n\nt1 <- torch_randint(low = 3, high = 7, size = c(3, 3, 3))\nt1$size()\n\n[1] 3 3 3\n\nt2 <- t1$unsqueeze(dim = 1)\nt2$size()\n\n[1] 1 3 3 3\n\nt3 <- t1$unsqueeze(dim = 2)\nt3$size()\n\n[1] 3 1 3 3\n\n\nPor otro lado, squeeze remueve las dimensiones de tamaño 1:\n\nt4 <- t3$squeeze()\nt4$size()\n\n[1] 3 3 3\n\n\nLo mismo puede conseguirse con view(), sin embargo, esta función es mucho mas general, aquí se permite reformar los datos a cualquier dimensionalidad válida (es decir que el número de elementos se mantiene igual).\nA continuación tenemos un tensor 3x2 que se reforma a una de tamaño 2x3:\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt1\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\n\nt2 <- t1$view(c(2, 3))\nt2\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPUFloatType{2,3} ]\n\n\nNótese que esto es diferente a transponer la matriz\nEn lugar de ir de 2 a 3 dimensiones, podemos “aplanar” una matriz a un vector.\n\nt4 <- t1$view(c(-1, 6))\n\nt4$size()\n\n[1] 1 6\n\nt4\n\ntorch_tensor\n 1  2  3  4  5  6\n[ CPUFloatType{1,6} ]\n\n\nEn contraste con las operaciones de indexación, aqui no se pierde dimensiones.\nComo se dijo anteriormente, las operaciones squeeze() o view() no crea copias. O dicho de otro modo: el tensor de salida comparte el almacenamineto con el tensor de entrada. Este hecho se puede verificar del siguiente modo:\n\nt1$storage()$data_ptr()\n\n[1] \"0x556940983e40\"\n\nt2$storage()$data_ptr()\n\n[1] \"0x556940983e40\"\n\n\nLo que difiere es los metadatos que torch mantiene acerca de los dos tensores. Aqui la información relevante es el paso:\nEl método stride() (paso) de un tensor revisa, para cada dimensión, cuantos elementos tienen que ser atravesados para llegar a su próximo elemento (fila o columna, en dos dimensiones). Para t1, de forma 3x2, tenemos que saltar sobre 2 elementos para llegar a la siguiente fila. Para llegar a la siguiente columna, solo tendríamos que saltar sobre un elemento:\n\nt1$stride()\n\n[1] 2 1\n\n\nPara t2, de la forma 3x2, la distancia entre los elementos columna es el mismo, pero la distancia entre filas es ahora 3:\n\nt2$stride()\n\n[1] 3 1\n\n\nMientras que las operaciones “cero-copia” son óptimas, hay casos donde no sirven.\nCon view(), puede ocurrir cuando un tensor obtenido vía una operación (diferente a view) que previamente haya modificado el stride o paso. Un ejemplo puede ser transpose():\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt1\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\n\nt1$stride()\n\n[1] 2 1\n\nt2 <- t1$t()\nt2\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPUFloatType{2,3} ]\n\nt2$stride()\n\n[1] 1 2\n\n\nEn el lenguaje de torch, los tensores (como t2) que están reutilizando cosas almacenadas previamente (solo que leídas de forma distinta), se dice que no son contiguas. Un modo de reformarlos es usar la función contiguous() previamente. Esto lo veremos en la siguiente sección.\n\n\nReformado con copia\nEn el siguiente fragmento de codigo se falla al intentar reformar t2 usando view(), dado que el tensor ya contiene información que indica que los datos no deben ser leidos en su orden fisico.\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\n\nt2 <- t1$t()\n\nt2$view(6)\n\nError in (function (self, size) : view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\nException raised from view_impl at ../aten/src/ATen/native/TensorShape.cpp:2674 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f384d8452eb in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0xd1 (0x7f384d840e41 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #2: at::native::view(at::Tensor const&, c10::ArrayRef<long>) + 0x325 (0x7f3820bad305 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #3: <unknown function> + 0x200b006 (0x7f382140b006 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #4: at::_ops::view::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>) + 0x98 (0x7f3821275718 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #5: <unknown function> + 0x37902f5 (0x7f3822b902f5 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #6: <unknown function> + 0x3790699 (0x7f3822b90699 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #7: at::_ops::view::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>) + 0x98 (0x7f3821275718 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #8: <unknown function> + 0x32b4588 (0x7f38226b4588 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #9: <unknown function> + 0x32b4a69 (0x7f38226b4a69 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #10: at::_ops::view::call(at::Tensor const&, c10::ArrayRef<long>) + 0xe7 (0x7f38212a1327 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #11: at::Tensor::view(c10::ArrayRef<long>) const + 0x42 (0x7f384e6050ce in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/liblantern.so)\nframe #12: _lantern_Tensor_view_tensor_intarrayref + 0x130 (0x7f384e3cf96d in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/liblantern.so)\nframe #13: cpp_torch_method_view_self_Tensor_size_IntArrayRef(XPtrTorchTensor, XPtrTorchIntArrayRef) + 0x35 (0x7f3851c2ff65 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #14: _torch_cpp_torch_method_view_self_Tensor_size_IntArrayRef + 0xa1 (0x7f3851996221 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #15: <unknown function> + 0xf7b6c (0x7f385caf7b6c in /usr/lib/R/lib/libR.so)\nframe #16: <unknown function> + 0xf80fd (0x7f385caf80fd in /usr/lib/R/lib/libR.so)\nframe #17: <unknown function> + 0x1317b5 (0x7f385cb317b5 in /usr/lib/R/lib/libR.so)\nframe #18: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #19: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #20: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #21: Rf_eval + 0x2ac (0x7f385cb4ea1c in /usr/lib/R/lib/libR.so)\nframe #22: <unknown function> + 0xc3227 (0x7f385cac3227 in /usr/lib/R/lib/libR.so)\nframe #23: <unknown function> + 0x1317b5 (0x7f385cb317b5 in /usr/lib/R/lib/libR.so)\nframe #24: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #25: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #26: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #27: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #28: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #29: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #30: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #31: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #32: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #33: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #34: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #35: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #36: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #37: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #38: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #39: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #40: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #41: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #42: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #43: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #44: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #45: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #46: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #47: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #48: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #49: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #50: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #51: Rf_eval + 0x2ac (0x7f385cb4ea1c in /usr/lib/R/lib/libR.so)\nframe #52: <unknown function> + 0x153fa7 (0x7f385cb53fa7 in /usr/lib/R/lib/libR.so)\nframe #53: <unknown function> + 0x1317b5 (0x7f385cb317b5 in /usr/lib/R/lib/libR.so)\nframe #54: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #55: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #56: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #57: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #58: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #59: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #60: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #61: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #62: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #63: <unknown function> + 0x14f294 (0x7f385cb4f294 in /usr/lib/R/lib/libR.so)\n\n\nSin embargo, si primero llamamos contiguous(), un nuevo tensor es creado, el cual podra ser (virtualmente) reformado usando view().\n\nt3 <- t2$contiguous()\n\nt3$view(6)\n\ntorch_tensor\n 1\n 3\n 5\n 2\n 4\n 6\n[ CPUFloatType{6} ]\n\n\nAlternativamente, podemos usar reshape(). Esta función se comportará similar a view() siempre que sea posible; de otro modo creará una copia física.\n\nt2$storage()$data_ptr()\n\n[1] \"0x556945ddf9c0\"\n\nt4 <- t2$reshape(6)\n\nt2$storage()$data_ptr()\n\n[1] \"0x556945ddf9c0\"\n\n\n\n\n\nOperaciones con tensores\nNo es para sorprenderse que torch provea una cantidad de operaciones con tensores; veremos algunos de ellos en el código de la red que se desarrollará luego y se encontrarán muchos mas con el uso de torch. Aquí echaremos un vistazo general a la semántica de los métodos de los tensores.\nLos métodos de los tensores normalmente retornan referencias a nuevos objetos. A continuación se suma a t1 un clon de si mismo:\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt2 <- t1$clone()\n\nt1$add(t2)\n\ntorch_tensor\n  2   4\n  6   8\n 10  12\n[ CPUFloatType{3,2} ]\n\n\nEn este proceso, t1 no ha sido modificado:\n\nt1\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\n\n\nMuchos métodos tienen variantes para operaciones de “mutación”. Todas estas incluyen un guion bajo:\n\nt1$add_(t1)\n\ntorch_tensor\n  2   4\n  6   8\n 10  12\n[ CPUFloatType{3,2} ]\n\n# Esta vez t1 es modificado\nt1\n\ntorch_tensor\n  2   4\n  6   8\n 10  12\n[ CPUFloatType{3,2} ]\n\n\nAlternativamente, se puede asignar el nuevo objeto a una nueva referencia de variable:\n\nt3 <- t1$add(t1)\n\nt3\n\ntorch_tensor\n  4   8\n 12  16\n 20  24\n[ CPUFloatType{3,2} ]\n\n\nTenemos ahora una cosa que discutir antes de cerrar esta introducción a los tensores: ¿Como podemos ejecutar todas estas operaciones en la GPU?\n\n\nEjecutando en la GPU\nPara verificar si hay una GPU visible para torch, ejecutar:\n\ncuda_is_available()\n\n[1] TRUE\n\ncuda_device_count()\n\n[1] 1\n\n\nLos tensores pueden ser almacenado en la GPU directamente desde su creación\n\ndevice <- torch_device(\"cuda\")\n\nt <- torch_ones(c(2, 2), device = device)\n\nTambién pueden ser movidos entre dispositivos en cualquier momento:\n\nt2 <- t$cuda()\nt2$device\n\ntorch_device(type='cuda', index=0)\n\nt3 <- t2$cpu()\nt3$device\n\ntorch_device(type='cpu')\n\n\nEstamos por concluir la discusión sobre tensores. Hay una característica mas de torch que, a pesar de estar relacionada con operaciones con tensores, merece una mención especial. Es conocida como broadcasting (difusión).\n\n\nBroadcasting\nA menudo ejecutamos operaciones en tensores cuyas formas no concuerdan con exactitud.\nPor ejemplo, podemos sumar un escalar con un tensor:\n\nt1 <- torch_randn(c(3, 5))\n\nt1+22\n\ntorch_tensor\n 23.3398  22.0977  21.5262  22.2440  20.0809\n 22.2100  21.8822  23.0445  21.1448  22.8481\n 22.9730  20.9074  22.9415  21.9149  21.4249\n[ CPUFloatType{3,5} ]\n\n\nTambién funciona si sumamos un tensor de tamaño 1\n\nt1 + torch_tensor(c(22))\n\ntorch_tensor\n 23.3398  22.0977  21.5262  22.2440  20.0809\n 22.2100  21.8822  23.0445  21.1448  22.8481\n 22.9730  20.9074  22.9415  21.9149  21.4249\n[ CPUFloatType{3,5} ]\n\n\nla suma de tensores de diferentes tamaños normalmente no funcionan:\n\nt1 <- torch_randn(c(3, 5))\nt2 <- torch_randn(c(5, 5))\n\nt1$add(t2)\n\nError in (function (self, other, alpha) : The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 0\nException raised from infer_size_impl at ../aten/src/ATen/ExpandUtils.cpp:35 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f384d8452eb in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xce (0x7f384d840cbe in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #2: at::infer_size_dimvector(c10::ArrayRef<long>, c10::ArrayRef<long>) + 0x48b (0x7f382061e32b in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #3: at::TensorIteratorBase::compute_shape(at::TensorIteratorConfig const&) + 0x10d (0x7f38206713cd in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #4: at::TensorIteratorBase::build(at::TensorIteratorConfig&) + 0x69 (0x7f3820672609 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #5: at::TensorIteratorBase::build_borrowing_binary_op(at::TensorBase const&, at::TensorBase const&, at::TensorBase const&) + 0xf7 (0x7f3820673de7 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #6: at::meta::structured_add_Tensor::meta(at::Tensor const&, at::Tensor const&, c10::Scalar const&) + 0x2f (0x7f382084183f in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #7: <unknown function> + 0x20415e6 (0x7f38214415e6 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #8: <unknown function> + 0x20416e6 (0x7f38214416e6 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #9: at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::Scalar const&) + 0x98 (0x7f3821148cf8 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #10: <unknown function> + 0x319a8ca (0x7f382259a8ca in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #11: <unknown function> + 0x319b049 (0x7f382259b049 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #12: at::_ops::add_Tensor::call(at::Tensor const&, at::Tensor const&, c10::Scalar const&) + 0x173 (0x7f3821177cc3 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #13: at::Tensor::add(at::Tensor const&, c10::Scalar const&) const + 0x3f (0x7f384e5fc0ef in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/liblantern.so)\nframe #14: _lantern_Tensor_add_tensor_tensor_scalar + 0x13f (0x7f384e173b3b in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/liblantern.so)\nframe #15: cpp_torch_method_add_self_Tensor_other_Tensor(XPtrTorchTensor, XPtrTorchTensor, XPtrTorchScalar) + 0x3b (0x7f3851c4fedb in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #16: _torch_cpp_torch_method_add_self_Tensor_other_Tensor + 0xb9 (0x7f3851945d99 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #17: <unknown function> + 0xf7b50 (0x7f385caf7b50 in /usr/lib/R/lib/libR.so)\nframe #18: <unknown function> + 0xf80fd (0x7f385caf80fd in /usr/lib/R/lib/libR.so)\nframe #19: <unknown function> + 0x1317b5 (0x7f385cb317b5 in /usr/lib/R/lib/libR.so)\nframe #20: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #21: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #22: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #23: Rf_eval + 0x2ac (0x7f385cb4ea1c in /usr/lib/R/lib/libR.so)\nframe #24: <unknown function> + 0xc3227 (0x7f385cac3227 in /usr/lib/R/lib/libR.so)\nframe #25: <unknown function> + 0x1317b5 (0x7f385cb317b5 in /usr/lib/R/lib/libR.so)\nframe #26: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #27: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #28: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #29: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #30: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #31: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #32: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #33: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #34: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #35: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #36: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #37: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #38: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #39: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #40: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #41: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #42: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #43: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #44: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #45: Rf_eval + 0x2ac (0x7f385cb4ea1c in /usr/lib/R/lib/libR.so)\nframe #46: <unknown function> + 0x153fa7 (0x7f385cb53fa7 in /usr/lib/R/lib/libR.so)\nframe #47: <unknown function> + 0x1317b5 (0x7f385cb317b5 in /usr/lib/R/lib/libR.so)\nframe #48: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #49: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #50: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #51: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #52: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #53: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\nframe #54: Rf_applyClosure + 0x1a5 (0x7f385cb512d5 in /usr/lib/R/lib/libR.so)\nframe #55: <unknown function> + 0x13ea30 (0x7f385cb3ea30 in /usr/lib/R/lib/libR.so)\nframe #56: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #57: <unknown function> + 0x14f294 (0x7f385cb4f294 in /usr/lib/R/lib/libR.so)\nframe #58: Rf_eval + 0x39e (0x7f385cb4eb0e in /usr/lib/R/lib/libR.so)\nframe #59: <unknown function> + 0x1547c0 (0x7f385cb547c0 in /usr/lib/R/lib/libR.so)\nframe #60: <unknown function> + 0x192f17 (0x7f385cb92f17 in /usr/lib/R/lib/libR.so)\nframe #61: <unknown function> + 0x130cc8 (0x7f385cb30cc8 in /usr/lib/R/lib/libR.so)\nframe #62: Rf_eval + 0x180 (0x7f385cb4e8f0 in /usr/lib/R/lib/libR.so)\nframe #63: <unknown function> + 0x15048f (0x7f385cb5048f in /usr/lib/R/lib/libR.so)\n\n\nSin embargo, bajo ciertas condiciones, uno o los dos tensores pueden ser expandidos virtualmente de forma que se alinean. Este comportamiento es lo que se denomina broadcasting. La forma en que esto funciona en torch no solo se inspira en NumPy, es idéntica.\nLas reglas son las siguientes:\n\nSe alinean las formas de los arreglos empezando desde la derecha: Digamos que se tienen dos tensores, uno de la forma 8x1x6x1 y otro de 7x1x5:\n\nforma t1: 8 1 6 1 forma t2: 7 1 5\n\nMirando desde la derecha, los tamaños a lo largo de los ejes alineados: o son iguales o uno de ellos es igual a 1, en cuyo caso el ultimo es ampliado al tamaño del mayor. En nuestro ejemplo tendríamos:\n\nforma t1: 8 1 6 1 forma t2: 7 6 5\nCon el broadcasting ocurriendo en t2.\n\nSi en el lado izquierdo, uno de los arreglos tiene un eje adicional (o mas de uno) el otro arreglo es virtualmente expandido para tener un tamaño de 1 es ese eje.\n\nforma t1: 8 1 6 1 forma t2: 1 7 1 5\ny luego ocurre el broadcast:\nforma t1: 8 1 6 1 forma t2: 8 7 1 5\nDe acuerdo con las anteriores reglas el ejemplo de sumar dos tensores de formas: 3x5 y 5x5 se podría modificar para permitir la suma de dos tensores.\nPor ejemplo, si t2 fuera 1x5, solo se requeriría ampliar a una forma de 3x5 antes de la operación suma:\n\nt1 <- torch_randn(c(3, 5))\nt2 <- torch_randn(c(1, 5))\n\nt1$add(t2)\n\ntorch_tensor\n 1.2613 -1.5797 -0.1304  0.8364  1.4811\n 2.0772 -0.5352 -0.1203 -0.3383  2.0498\n 3.3027 -1.4875 -0.3475 -0.1763  1.0905\n[ CPUFloatType{3,5} ]\n\n\nSi fuera de tamaño 5, una dimensión antecesora virtual podría ser añadida y entonces, el mismo broadcasting podría tomar lugar como en el caso anterior.\n\nt1 <- torch_randn(c(3, 5))\nt2 <- torch_randn(c(5))\n\nt1$add(t2)\n\ntorch_tensor\n 0.6716  0.7809 -0.6019 -0.6897 -0.2175\n-2.2945  2.2061 -2.8415  3.0838 -1.1119\n 0.0464 -0.0846  0.1769  0.8541 -0.1863\n[ CPUFloatType{3,5} ]\n\n\nA continuación un ejemplo mas complejo. Como ocurre un broadcasting en t1 y t2:\n\nt1 <- torch_randn(c(1, 5))\nt2 <- torch_randn(c(3, 1))\n\nt1$add(t2)\n\ntorch_tensor\n 0.3926 -0.4045 -0.5845 -1.5759  0.9646\n 0.0973 -0.6997 -0.8798 -1.8711  0.6694\n 1.8763  1.0792  0.8991 -0.0922  2.4483\n[ CPUFloatType{3,5} ]\n\n\nComo ejemplo conclusivo, un producto exterior se puede computar a traves de broadcasting como sigue:\n\nt1 <- torch_tensor(c(0, 10, 20, 30))\nt2 <- torch_tensor(c(1, 2, 3))\n\nt1$view(c(4, 1)) * t2\n\ntorch_tensor\n  0   0   0\n 10  20  30\n 20  40  60\n 30  60  90\n[ CPUFloatType{4,3} ]\n\n\nAhora si, estamos listos para implementar una red neuronal!"
  },
  {
    "objectID": "posts/Tensores/index.html#red-neuronal-simple-usando-tensores",
    "href": "posts/Tensores/index.html#red-neuronal-simple-usando-tensores",
    "title": "Familiarizandose con tensores en torch",
    "section": "Red Neuronal Simple Usando Tensores",
    "text": "Red Neuronal Simple Usando Tensores\nNuestra tarea, para la cual sera usado una aproximación de bajo nivel y que será simplificada considerablemente en próximos desarrollos, consiste en hacer la regresión de una variable de salida basados en tres variables de entrada.\nSe usa torch directamente para simular algunos datos.\n\nDatos\n\n# dimensión de la entrada\nd_in <- 3\n# dimensión de la salida\nd_out <- 1\n# cantidad de observaciones en el conjunto de entrenamiento\nn <- 100\n\n# Creación de datos aleatorios\n# Entrada\nx <- torch_randn(n, d_in)\n# Salida\ny <- x[, 1, drop = F] * 0.2 -\n  x[, 2, drop = F] * 1.3 -\n  x[, 3, drop = F] * 0.5 +\n  torch_randn(n, 1)\n\nAhora, se requiere inicializar los pesos de la red. Tendremos una capa oculta con 32 unidades. El tamaño de la capa de salida, determinada por la tarea, es igual a 1.\n\n\nInicializar Pesos\n\n# dimensiones del la capa oculta\nd_hidden <- 32\n\n# Pesos que conectan la entrada con la capa oculta\nw1 <- torch_randn(d_in, d_hidden)\n# Pesos que conectan la capa oculta con la salida\nw2 <- torch_randn(d_hidden, d_out)\n\n# sesgos de la capa oculta\nb1 <- torch_zeros(1, d_hidden)\n# sesgos de la salida\nb2 <- torch_zeros(1, d_out)\n\nAhora vamos a hacer el ciclo de entrenamiento propiamente. El ciclo de entrenamiento es, en realidad, la red neuronal.\n\n\nCiclo de entrenamiento\nEn cada iteración (época), el ciclo de entrenamiento hace cuatro cosas:\n\nSe hace la propagación hacia adelante, se computa las predicciones\nSe comparan las predicciones con las salidas reales y se cuantifica la perdida\nse hace la propagación hacia atrás en la red, se calculan los gradientes que indican como deben cambiarse los pesos\nSe actualizan los pesos, haciendo uso de la tasa de aprendizaje.\n\nEl formato seria como se muestra a continuación:\n\nfor (t in 1:200) {\n  ### -------------- Propagación hacia adelante ---------------\n  # Aquí vamos a calcular la predicción\n  \n  \n  ### -------------- Calculo de la perdida --------------------\n  # Aquí vamos a calcular la suma de los errores al cuadrado\n  \n  \n  ### -------------- Propagación hacia atrás ------------------\n  # Aquí vamos a propagar hacia atrás para calcular los gradientes\n  \n  \n  ### -------------- Actualización de los pesos ---------------\n  # Aquí vamos a actualizar los pesos, substrayendo una porción de los gradientes\n  \n  \n}\n\nLa propagación hacia adelante efectúa dos transformaciones afines, una para la capa oculta y otra para la capa de salida. En el intermedio se aplica una activación ReLU:\n\n# calculo de las pre-activaciones de las capas ocultas (dim: 100 x 32)\n# torch_mm hace multiplicación de matrices\nh <- x$mm(w1) + b1\n\n# se aplica la función de activación (dim: 100 x 32)\n# torch_clamp corta los valores arriba/abajo de limites dados\nh_relu <- h$clamp(min = 0)\n\n# Calculo de la salida (dim: 100 x 1)\ny_pred <- h_relu$mm(w2) + b2\n\nNuestra función de perdidas es el error cuadrático medio\n\nloss <- as.numeric((y_pred - y)$pow(2)$sum())\n\nEl calculo manual de los gradientes es un poco tedioso, pero puede ser realizado:\n\n# gradiente de perdidas w.r.t predicción (dim: 100 x 1)\ngrad_y_pred <- 2 * (y_pred - y)\n# gradiente de perdidas w.r.t w2 (dim: 32 x 1)\ngrad_w2 <- h_relu$t()$mm(grad_y_pred)\n# gradiente de perdidas w.r.t activación capa oculta (dim: 100 x 32)\ngrad_h_relu <- grad_y_pre$mm(w2$t())\n# gradiente de perdidas w.r.t pre-activación capa oculta (dim: 100 x 32)\ngrad_h <- grad_h_relu$clone()\n\ngrad_h[h < 0] <- 0\n\n# gradiente de perdidas w.r.t b2 (forma: ())\ngrad_b2 <- grad_y_pred$sum()\n\n# gradiente de perdidas w.r.t w1 (dim 3 x 32)\ngrad_w1 <- x$t()$mm(grad_h)\n# gradiente de perdidas w.r.t b1 (forma: (32, ))\ngrad_b1 <- grad_h$sum(dim = 1)\n\nEl ultimo paso es entonces usar los gradientes calculado para actualizar los pesos:\n\nlearning_rate <- 1e-4\n\nw2 <- w2 - learning_rate * grad_w2\nb2 <- b2 - learning_rate * grad_b2\nw1 <- w1 - learning_rate * grad_w1\nb1 <- b1 - learning_rate * grad_b1\n\nUsando estos fragmentos de código podemos ahora llenar el formato anterior y hacer pruebas!\n\nlibrary(torch)\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <-\n  x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n### initialize weights ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden)\n# output layer bias\nb2 <- torch_zeros(1, d_out)\n\n### network parameters ---------------------------------------------------------\n\nlearning_rate <- 1e-4\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  ### -------- Forward pass --------\n  \n  # compute pre-activations of hidden layers (dim: 100 x 32)\n  h <- x$mm(w1) + b1\n  # apply activation function (dim: 100 x 32)\n  h_relu <- h$clamp(min = 0)\n  # compute output (dim: 100 x 1)\n  y_pred <- h_relu$mm(w2) + b2\n  \n  ### -------- compute loss --------\n\n  loss <- as.numeric((y_pred - y)$pow(2)$sum())\n  \n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss, \"\\n\")\n  \n  ### -------- Backpropagation --------\n  \n  # gradient of loss w.r.t. prediction (dim: 100 x 1)\n  grad_y_pred <- 2 * (y_pred - y)\n  # gradient of loss w.r.t. w2 (dim: 32 x 1)\n  grad_w2 <- h_relu$t()$mm(grad_y_pred)\n  # gradient of loss w.r.t. hidden activation (dim: 100 x 32)\n  grad_h_relu <- grad_y_pred$mm(\n    w2$t())\n  # gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)\n  grad_h <- grad_h_relu$clone()\n  \n  grad_h[h < 0] <- 0\n  \n  # gradient of loss w.r.t. b2 (shape: ())\n  grad_b2 <- grad_y_pred$sum()\n  \n  # gradient of loss w.r.t. w1 (dim: 3 x 32)\n  grad_w1 <- x$t()$mm(grad_h)\n  # gradient of loss w.r.t. b1 (shape: (32, ))\n  grad_b1 <- grad_h$sum(dim = 1)\n  \n  ### -------- Update weights --------\n  \n  w2 <- w2 - learning_rate * grad_w2\n  b2 <- b2 - learning_rate * grad_b2\n  w1 <- w1 - learning_rate * grad_w1\n  b1 <- b1 - learning_rate * grad_b1\n  \n}\n\nEpoch:  10    Loss:  239.8459 \nEpoch:  20    Loss:  175.6584 \nEpoch:  30    Loss:  142.8526 \nEpoch:  40    Loss:  123.4776 \nEpoch:  50    Loss:  110.2153 \nEpoch:  60    Loss:  101.4229 \nEpoch:  70    Loss:  95.29362 \nEpoch:  80    Loss:  90.94321 \nEpoch:  90    Loss:  87.41534 \nEpoch:  100    Loss:  84.34146 \nEpoch:  110    Loss:  81.86013 \nEpoch:  120    Loss:  79.76909 \nEpoch:  130    Loss:  77.94298 \nEpoch:  140    Loss:  76.36224 \nEpoch:  150    Loss:  74.9823 \nEpoch:  160    Loss:  73.76436 \nEpoch:  170    Loss:  72.68542 \nEpoch:  180    Loss:  71.69421 \nEpoch:  190    Loss:  70.77998 \nEpoch:  200    Loss:  69.95052 \n\n\nParece que funciona bastante bien! También se ha cumplido con el propósito inicial: mostrar todo lo que se puede conseguir usando únicamente tensores con torch. En caso que no te sientas entusiasmado con el desarrollo de la lógica de propagación hacia atrás, no te preocupes, en la próxima entrega esto será significativamente menos exigente. Nos veremos entonces!"
  },
  {
    "objectID": "posts/autograd/index.html",
    "href": "posts/autograd/index.html",
    "title": "Presentando el autograd de torch",
    "section": "",
    "text": "Anteriormente se vio como programar una red neuronal simple desde el principio usando únicamente tensores de torch. Las predicciones, perdidas, gradientes, actualizaciones en los pesos, todas estas cosas las calculamos nosotros mismos. Ahora vamos a hacer un avance significativo: vamos a ahorrarnos el calculo de los gradientes y tenemos a torch para que haga eso por nosotros.\nPero antes, debemos obtener un poco de contexto."
  },
  {
    "objectID": "posts/autograd/index.html#diferenciación-automatica-con-autograd",
    "href": "posts/autograd/index.html#diferenciación-automatica-con-autograd",
    "title": "Presentando el autograd de torch",
    "section": "Diferenciación automatica con autograd",
    "text": "Diferenciación automatica con autograd\ntorch usa un módulo llamado autograd para:\n\nRegistrar las operaciones aplicadas a los tensores y\nAlmacenar lo que se ha hecho para obtener los gradientes correspondientes, una vez que se ha entrado en la fase de retro-propagación.\n\nEstas acciones previas son almacenadas internamente como funciones y cuando es el momento de calcular los gradientes, estas funciones se aplican en orden: se inicia desde el nodo de salida y se calculan los gradientes sucesivamente en retro-propagación a través de la red. Es una forma de modo reverso de diferenciación automática.\n\nBases del autograd\nComo usuarios, podemos ver un poco de la implementación. Como prerequisito para que el registro ocurra, los tensores tiene que ser creados con requires_grad = TRUE. Por ejemplo:\n\nlibrary(torch)\n\nx <- torch_ones(2, 2, requires_grad = TRUE)\n\nPara ser claros, x es ahora un tensor con respecto al cual se tiene que calcular el gradiente, normalmente un tensor que representa pesos o sesgos 1, no los datos de entrada. Si nosotros aplicamos subsecuentemente una operación a dicho tensor y asignamos el resultado a y:\n\ny <- x$mean()\n\nAhora encontramos que y tiene un registro no-vacio en grad_fn que indica como debe calcularse el gradiente de y en el punto x:\n\ny$grad_fn\n\nMeanBackward0\n\n\nEl cálculo de los gradientes se hace en el llamado a la función backward() en el tensor de salida.\n\ny$backward()\n\nDespués de backward(), x tiene un campo no-nulo llamado gradque almacena el gradiente de y en el punto x:\n\nx$grad\n\ntorch_tensor\n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\n\n\nEn cadenas mas largas de cómputos, podemos revisar como torch construye un grafo de operaciones backward. A continuación tenemos un ejemplo un poco mas complejo, siéntase libre de saltarlo si no es del tipo que inspeccionar los detalles para que la cosas tengan sentido.\n\n\nProfundizando\nSe construye un grafo simple de tensores, con entradas x1y x2 siendo conectadas a la salida out por intermedio de y y z.\n\nx1 <- torch_ones(2, 2, requires_grad = TRUE)\nx2 <- torch_tensor(1.1, requires_grad = TRUE)\n\ny <- x1*(x2+2)\nz <- y$pow(2)*3\nout <- z$mean()\n\nPara ahorrar memoria, los gradientes intermedios normalmente no son almacenados. Llamando a retain_grad() en un tensor nos permite cambiar la opción predeterminada. Hagamoslo aquí a modo de demostración:\n\ny$retain_grad()\nz$retain_grad()\n\nAhora podemos revisar el grafo para inspeccionar el plan de acción de torch para la retro-progagación, empezando desde out$grad_fn:\n\n# cómo se calcula el gradiente para el promedio, la ultima acción ejecutada\nout$grad_fn\n\nMeanBackward0\n\n\n\n# cómo se calcula el gradiente para la multiplicación por 3 en z = y.pow(2)*3\nout$grad_fn$next_functions\n\n[[1]]\nMulBackward1\n\n\n\n# cómo se calcula el gradiente para pow en z = y.pow(2)*3\nout$grad_fn$next_functions[[1]]$next_functions\n\n[[1]]\nPowBackward0\n\n\n\n# cómo se calcula el gradiente para la multiplicación en y = x*(x+2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions\n\n[[1]]\nMulBackward0\n\n\n\n# cómo se calcula el gradiente de las dos ramas de y = x*(x+2), donde el camino izquierdo es un nodo hoja (AccumulateGrad para x1)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions\n\n[[1]]\ntorch::autograd::AccumulateGrad\n[[2]]\nAddBackward1\n\n\n\n# Aquí llegamos a otro nodo hoja (AccumulateGrad para x2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions[[2]]$next_functions\n\n[[1]]\ntorch::autograd::AccumulateGrad\n\n\nSi llamamos ahora out$backward(), todos los tensores en el grafo tendrán sus respectivos gradientes calculados.\n\nout$backward()\n\nz$grad\n\ntorch_tensor\n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\n\ny$grad\n\ntorch_tensor\n 4.6500  4.6500\n 4.6500  4.6500\n[ CPUFloatType{2,2} ]\n\nx2$grad\n\ntorch_tensor\n 18.6000\n[ CPUFloatType{1} ]\n\nx1$grad\n\ntorch_tensor\n 14.4150  14.4150\n 14.4150  14.4150\n[ CPUFloatType{2,2} ]\n\n\nDespués de esta revisión, observemos ahora como autograd hace nuestra red neuronal mas simple."
  },
  {
    "objectID": "posts/autograd/index.html#red-neuronal-usando-autograd",
    "href": "posts/autograd/index.html#red-neuronal-usando-autograd",
    "title": "Presentando el autograd de torch",
    "section": "Red Neuronal, usando autograd",
    "text": "Red Neuronal, usando autograd\nGracias a autograd podemos despedirnos del proceso tedioso y propenso a errores de programar la retro-propagación por nosotros mismos. Un único método hace todo esto: loss$backward().\nCon torch registrando las operaciones, no se requiere nombrar explícitamente los tensores intermedios. Podemos programar la propagación hacia adelante, hacer el calculo de las perdidas y la retro-propagación en solo tres lineas:\n\ny_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\nloss <- (y_pred - y)$pow(2)$sum()\nloss$backward\n\nA continuación el código completo. Estamos en una fase intermedia: Aun tenemos que calcular manualmente la propagación hacia adelante y las perdidas y aun tenemos que actualizar manualmente los pesos. Por esto último, hay algo que requiere ser explicado, pero revisemos primero la nueva versión de la red neuronal:\n\nlibrary(torch)\n\n### Generación de los datos de entrenamiento\n\n# Dimensión de la entrada (número de características de entrada)\nd_in <- 3\n# Dimensión de la salida (número de características predecidas)\nd_out <- 1\n# Número de observaciones del conjunto de entrenamiento\nn <- 100\n\n# Creación de datos aleatorios\nx <- torch_randn(n, d_in)\ny <- x[, 1, drop = F] * 0.2 - x[, 2, drop = F] *1.3 - x[, 3, drop = F] * 0.5 + torch_randn(n, 1)\n\n### inicialización de los pesos\n\n# Dimensiones de la capa oculta\nd_hidden <- 32\n# Pesos que conectan la entrada con la capa oculta\nw1 <- torch_randn(d_in, d_hidden, requires_grad = T)\n# Pesos que conectan la capa oculta con la salida\nw2 <- torch_randn(d_hidden, d_out, requires_grad = T)\n\n# Sesgos de la capa oculta\nb1 <- torch_zeros(1, d_hidden, requires_grad = T)\n# Sesgos de la capa de salida\nb2 <- torch_zeros(1, d_out, requires_grad = T)\n\n### Parámetros de la red\n\nlearning_rate <- 1e-4\n\n### Ciclo de entrenamiento\n\nfor(t in 1:200){\n  ### Propagación hacia adelante\n  y_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\n  # clamp simula \n  \n  ### Cálculo de la perdida\n  loss <- (y_pred-y)$pow(2)$sum()\n  if(t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### Retro-propagación\n  \n  # cálculo del gradiente, todos los tensores con requires_grad = TRUE\n  loss$backward()\n  \n  ### Actualización de los pesos\n  \n  # se ejecuta con with_no_grad() porque esta parte no queremos que se \n  # haga calculo automático del gradiente\n  with_no_grad({\n    w1 <- w1$sub_(learning_rate * w1$grad)\n    w2 <- w2$sub_(learning_rate * w2$grad)\n    b1 <- b1$sub_(learning_rate * b1$grad)\n    b2 <- b2$sub_(learning_rate * b2$grad)\n    # el método sub_ parece que resta al tensor original el argumento\n    \n    # Actualización de los gradientes después de cada ciclo, de \n    # otro modo se acumularían\n    w1$grad$zero_()\n    w2$grad$zero_()\n    b1$grad$zero_()\n    b2$grad$zero_()\n  })\n  \n}\n\nEpoch:  10    Loss:  176.326 \nEpoch:  20    Loss:  126.9562 \nEpoch:  30    Loss:  115.167 \nEpoch:  40    Loss:  110.2214 \nEpoch:  50    Loss:  107.4412 \nEpoch:  60    Loss:  104.9799 \nEpoch:  70    Loss:  102.9808 \nEpoch:  80    Loss:  101.2993 \nEpoch:  90    Loss:  99.50574 \nEpoch:  100    Loss:  97.84877 \nEpoch:  110    Loss:  96.35872 \nEpoch:  120    Loss:  95.11468 \nEpoch:  130    Loss:  94.0111 \nEpoch:  140    Loss:  92.97823 \nEpoch:  150    Loss:  91.99876 \nEpoch:  160    Loss:  90.98146 \nEpoch:  170    Loss:  90.04476 \nEpoch:  180    Loss:  89.19314 \nEpoch:  190    Loss:  88.43552 \nEpoch:  200    Loss:  87.65956 \n\n\nComo se explicó antes, después de algun_tensor$backward(), todos los tensores precedentes en el grafo actualizarán los campos grad2. Nosotros hacemos uso de esos campos para actualizar los pesos, pero ahora autograd esta “encendido”, siempre que se ejecute una operación esta quedará registrada para la retro-propagación, por lo cual se requiere desactivar el autogradde forma explicita usando with_no_grad().\nMientras que esto es algo que podríamos clasificar como “bueno saberlo”, dado que una vez lleguemos al último documento de la serie, la actualización de los pesos también será automatizada, el concepto de reiniciar en cero los gradientes esta aquí para quedarse: los valores almacenados en los campos gradse acumulan, en cualquier parte donde se usen, será necesario reiniciarlos en cero para reutilizarlos.\n\nEn resumen\nEn donde quedamos? Se inicio con la programación de la red neuronal completamente de cero, únicamente usando tensores. Ahora obtuvimos una ayuda significativa usando autograd.\nPero aun estamos usando la actualización manual de los pesos y aun no tenemos abstracciones que nos permitan un fácil desarrollo de arquitecturas de aprendizaje profundo (“Capas” o “Módulos”).\nAmbos temas serán tratados en las próximas entregas. Gracias por leernos!"
  },
  {
    "objectID": "posts/modulos/index.html",
    "href": "posts/modulos/index.html",
    "title": "Usando módulos de torch",
    "section": "",
    "text": "Inicialmente, empezamos aprendiendo sobre las bases de torch programando una red neuronal sencilla “desde el principio”, haciendo uso solamente de una de las características de torch: los tensores. Ahora, vamos a simplificar muchísimo la tarea reemplazando la retro-propagación con autograd. Hoy vamos a modularizar la red en dos sentidos: en el habitual y en uno mucho mas literal: las operaciones de matrices de bajo nivel son reemplazadas por módulos de torch."
  },
  {
    "objectID": "posts/modulos/index.html#módulos",
    "href": "posts/modulos/index.html#módulos",
    "title": "Usando módulos de torch",
    "section": "Módulos",
    "text": "Módulos\nEn otras plataformas (por ejemplo keras), se puede estar acostumbrado a distinguir entre módulos y capas. En torch, ambos son instancias de nn_module(), y por lo tanto, se tienen algunos métodos en común. Para aquellos que piensan en términos de “modelos” y “capas”, se dividió artificialmente esta sección en dos partes. En realidad no hay dicotomía: nuevos módulos pueden estar compuestos de módulos existentes en niveles arbitrarios de recursión.\n\nModulos Base (Capas)\nEn lugar de escribir una transformación afin manualmente: x$mm(w1) + b1 por ejemplo, como lo hemos hecho hasta ahora, podemos crear un modulo lineal. El siguiente fragmento de código crea una instancia de una capa lineal que espera como entrada tres variables y devuelve una salida por observación:\n\nlibrary(torch)\nl <- nn_linear(3, 1)\n\nEl módulo tiene dos parámetros, “Peso” y “Sesgo”. Ambos vienen pre-inicializados:\n\nl$parameters\n\n$weight\ntorch_tensor\n 0.1647  0.5118 -0.1313\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n 0.4457\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nLos módulos pueden ser invocados; la invocación de un módulo ejecuta el método forward(), el cual, para una capa lineal, multiplica (matricialmente) la entrada por los pesos y suma el sesgo.\nIntentemos lo siguiente:\n\ndata <- torch_randn(10, 3)\nout <- l(data)\n\nComo se esperaba, la salida out ahora contiene algunos datos:\n\nout$data()\n\ntorch_tensor\n 0.8072\n 0.1507\n 0.9358\n 0.2930\n 0.7551\n 0.1299\n-0.4496\n-0.2296\n-0.2524\n 0.5204\n[ CPUFloatType{10,1} ]\n\n\nAdicionalmente, este tensor sabe que requiere ser hecho siempre que se le pida calcular gradientes:\n\nout$grad_fn\n\nAddmmBackward0\n\n\nNótese la diferencia entre los tensores retornados por los módulos y los creados por comandos nuestros. Cuando creamos los tensores, se requiere definir requires_grad = TRUE para activar el calculo de gradientes. Con los módulos, torch asume correctamente que deseamos realizar la retro-propagación en algún momento.\nPor ahora, no hemos invocado backward() aun. Así que aún no se ha calculado ningún gradiente:\n\nl$weight$grad\n\ntorch_tensor\n[ Tensor (undefined) ]\n\nl$bias$grad\n\ntorch_tensor\n[ Tensor (undefined) ]\n\n\nCambiemos esto:\n\nout$backward()\n\nError in (function (self, inputs, gradient, retain_graph, create_graph) : grad can be implicitly created only for scalar outputs\nException raised from _make_grads at ../torch/csrc/autograd/autograd.cpp:47 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7fd6b34452eb in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0xd1 (0x7fd6b3440e41 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libc10.so)\nframe #2: <unknown function> + 0x38f064f (0x7fd6888f064f in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #3: torch::autograd::backward(std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, c10::optional<bool>, bool, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) + 0x45 (0x7fd6888f2035 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #4: <unknown function> + 0x395ccfe (0x7fd68895ccfe in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #5: at::Tensor::_backward(c10::ArrayRef<at::Tensor>, c10::optional<at::Tensor> const&, c10::optional<bool>, bool) const + 0x49 (0x7fd6862fc8d9 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/./libtorch_cpu.so)\nframe #6: _lantern_Tensor__backward_tensor_tensorlist_tensor_bool_bool + 0x17b (0x7fd6b3d4d035 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/lib/liblantern.so)\nframe #7: <unknown function> + 0x5947c5 (0x7fd6b77947c5 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #8: std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, std::__future_base::_Task_state<std::function<void ()>, std::allocator<int>, void ()>::_M_run()::{lambda()#1}, void> >::_M_invoke(std::_Any_data const&) + 0x35 (0x7fd6b77965b5 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #9: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*) + 0x2d (0x7fd6b77968ed in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #10: <unknown function> + 0x99f68 (0x7fd6c2899f68 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #11: EventLoop<void>::run() + 0x1a4 (0x7fd6b7798634 in /home/jhonatan/R/x86_64-pc-linux-gnu-library/4.1/torch/libs/torchpkg.so)\nframe #12: <unknown function> + 0xdc2b3 (0x7fd6bfedc2b3 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #13: <unknown function> + 0x94b43 (0x7fd6c2894b43 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #14: <unknown function> + 0x126a00 (0x7fd6c2926a00 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\n\n¿Porque se produce un error? autograd espera que el tensor de salida sea un escalar, mientras que en nuestro ejemplo tenemos un tensor de tamaño (10, 1). Este error no ocurriría en la practica, donde se trabaja por baches de entradas (en ocasiones, solo un único bache). Pero aun así, es interesante ver como resolver esto.\nPara hacer que nuestro ejemplo funcione, se introduce un paso adicional, calculo de una media virtual. LLamemoslo avg. Si tal media fuera calculada, su gradiente con respecto a l$weight podría ser obtenida vía regla de la cadena:\n\\[\n\\frac{\\partial avg}{\\partial w} = \\frac{\\partial avg}{\\partial out}\n\\frac{\\partial out}{\\partial w}\n\\]\nDe las cantidades de la derecha, estamos interesados en la segunda. Se necesita proveer la primera, de la forma en que esto podría verse si realmente estuviéramos calculando la media:\n\nd_avg_d_out <- torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t()\nout$backward(gradient = d_avg_d_out)\n\nAhora, l$wieght$grad y l$bias$grad sí contienen los gradientes:\n\nl$weight$grad\n\ntorch_tensor\n-27.2703 -23.9543   9.2242\n[ CPUFloatType{1,3} ]\n\nl$bias$grad\n\ntorch_tensor\n 100\n[ CPUFloatType{1} ]\n\n\nAdicionalmente a nn_linear(), torch provee mucho de todo los que se espera de las capas mas comunes. Aún así, algunas tareas se resuelven por una sola capa ¿como combinarlas? o, en lenguaje común: ¿como construir modelos?\n\n\nMódulos contenedores (“Modelos”)\nBien, los modelos son módulos que contienen otros módulos. Por ejemplo, si todas las entradas se supone que fluyen atraves de los mismos nodos y a lo largo de las mismas vías, entonces nn_sequentiual() puede usarse para construir un grafo sencillo.\nPor ejemplo:\n\nmodel <- nn_sequential(\n  nn_linear(3, 16),\n  nn_relu(),\n  nn_linear(16, 1)\n)\n\npodemos usar la misma técnica de arriba para ver los parámetros del modelo (dos matrices de pesos y dos vectores de sesgo):\n\nmodel$parameters\n\n$`0.weight`\ntorch_tensor\n 0.5355 -0.0213  0.4647\n 0.1244 -0.4988 -0.4234\n 0.2565  0.2977  0.2528\n 0.5149 -0.0701 -0.0679\n-0.4810  0.1394 -0.2943\n-0.4708  0.4047 -0.5536\n 0.0525 -0.4987  0.3909\n-0.4642  0.2297 -0.1231\n-0.4173 -0.2474 -0.4889\n-0.2684 -0.4513  0.2920\n 0.3599 -0.5580 -0.0656\n-0.5073  0.0704  0.4467\n 0.1711  0.5273  0.2958\n-0.0286 -0.4735 -0.1189\n 0.0977  0.2855 -0.2965\n 0.0773 -0.3401  0.5104\n[ CPUFloatType{16,3} ][ requires_grad = TRUE ]\n\n$`0.bias`\ntorch_tensor\n 0.1984\n 0.5505\n-0.4983\n 0.3101\n-0.3564\n 0.0365\n-0.4945\n 0.3637\n 0.1587\n 0.5755\n 0.1083\n 0.2867\n 0.0973\n-0.2592\n 0.1121\n-0.0347\n[ CPUFloatType{16} ][ requires_grad = TRUE ]\n\n$`2.weight`\ntorch_tensor\nColumns 1 to 10-0.2272 -0.0307  0.1232 -0.1382 -0.0439  0.1983  0.1353  0.0458 -0.2376  0.2480\n\nColumns 11 to 16 0.2444 -0.1623 -0.0844 -0.0362 -0.1659 -0.0595\n[ CPUFloatType{1,16} ][ requires_grad = TRUE ]\n\n$`2.bias`\ntorch_tensor\n 0.1119\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nPara inspeccionar un parámetro individual, hay que usar la posición en el modelo secuencial. Por ejemplo:\n\nmodel[[1]]$bias\n\ntorch_tensor\n 0.1984\n 0.5505\n-0.4983\n 0.3101\n-0.3564\n 0.0365\n-0.4945\n 0.3637\n 0.1587\n 0.5755\n 0.1083\n 0.2867\n 0.0973\n-0.2592\n 0.1121\n-0.0347\n[ CPUFloatType{16} ][ requires_grad = TRUE ]\n\n\nY del mismo modo que antes en nn_linear(), este módulo puede ser invocado directamente sobre los datos:\n\nout <- model(data)\n\nEn un módulo compuesto (modelo) como este, invocar backward() hará la retro-propagación a través de todas las capas:\n\nout$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t())\n\nmodel[[1]]$bias$grad\n\ntorch_tensor\n-13.6346\n -2.7662\n  0.0000\n -6.9083\n -1.3165\n  5.9494\n  4.0576\n  3.2053\n-16.6351\n 22.3234\n 14.6633\n -9.7406\n -3.3776\n -1.4462\n -9.9561\n -4.1668\n[ CPUFloatType{16} ]\n\n\nY ubicando el módulo compuesto (modelo) en la GPU moverá todos los tensores allí:\n\nmodel$cuda()\nmodel[[1]]$bias$grad\n\ntorch_tensor\n-13.6346\n -2.7662\n  0.0000\n -6.9083\n -1.3165\n  5.9494\n  4.0576\n  3.2053\n-16.6351\n 22.3234\n 14.6633\n -9.7406\n -3.3776\n -1.4462\n -9.9561\n -4.1668\n[ CUDAFloatType{16} ]\n\n\nVeamos ahora cómo, usando nn_sequential() se puede simplificar nuestra red neuronal de ejemplo."
  },
  {
    "objectID": "posts/modulos/index.html#red-neuronal-simple-usando-módulos",
    "href": "posts/modulos/index.html#red-neuronal-simple-usando-módulos",
    "title": "Usando módulos de torch",
    "section": "Red neuronal simple usando módulos",
    "text": "Red neuronal simple usando módulos\n\n### generación de datos de entrenamiento ---------------\n\n# dimensiones de la entrada (número de características de entrada)\nd_in <- 3\n# dimensiones de la salida (número de características de predicción)\nd_out <- 1\n# número de observaciones en el conjunto de entrenamiento\nn <- 100\n\n# Creación de datos aleatorios\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n# Nótese que se usan NULL, pero podría reemplazarse por el parámetro drop = FALSE, sirve para asegurarse que no se pierde las dimensiones originales de los tensores al hacer la selección\n\n### Definición de la red neuronal\n\n# dimensiones de la capa oculta\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### Parámetros de la red\n\nlearning_rate <- 1e-4\n\n### Ciclo de entrenamiento\n\nfor (t in 1:200){\n  ### ------ propagación hacia adelante---------\n  \n  y_pred <- model(x)\n  \n  ### ------ cálculo de perdidas --------\n  \n  loss <- (y_pred - y)$pow(2)$sum()\n  if(t %% 10 ==0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### ------- retro-propagación ---------\n  \n  # puesta a cero de los gradientes antes de iniciar la retro-propagación\n  model$zero_grad()\n  \n  # Cálculo de los gradientes para los parámetros del modelo\n  loss$backward()\n  \n  ### ------- actualizacion de los pesos -------\n  # se ejecuta con with_no_grad() porque en esta parte no se desea almacenar el calculo\n  # automático del gradiente\n  # Se actualiza cada parámetro con su respectivo `grad`\n  \n  with_no_grad({\n    model$parameters %>% purrr::walk(function(param) param$sub_(learning_rate * param$grad))\n  })\n}\n\nEpoch:  10    Loss:  248.7436 \nEpoch:  20    Loss:  178.2965 \nEpoch:  30    Loss:  141.0255 \nEpoch:  40    Loss:  122.7597 \nEpoch:  50    Loss:  114.196 \nEpoch:  60    Loss:  110.0747 \nEpoch:  70    Loss:  107.8717 \nEpoch:  80    Loss:  106.5085 \nEpoch:  90    Loss:  105.5374 \nEpoch:  100    Loss:  104.7832 \nEpoch:  110    Loss:  104.1391 \nEpoch:  120    Loss:  103.556 \nEpoch:  130    Loss:  102.988 \nEpoch:  140    Loss:  102.452 \nEpoch:  150    Loss:  101.9532 \nEpoch:  160    Loss:  101.4878 \nEpoch:  170    Loss:  101.0473 \nEpoch:  180    Loss:  100.6296 \nEpoch:  190    Loss:  100.2312 \nEpoch:  200    Loss:  99.85239 \n\n\nLa propagación hacia adelante se ve mucho mas simple ahora; sin embargo, aun tenemos que hacer el ciclo sobre los parámetros del modelo y la actualización de cada uno manualmente. Posiblemente, usted puede sospechar que torch provee abstracciones para funciones comunes de funciones de perdidas. En la próxima entrega de esta serie (que ademas será la final), vamos a tratar estos dos puntos, haciendo uso de las perdidas y optimizadores de torch. Nos veremos entonces!"
  },
  {
    "objectID": "posts/modulos/index.html#introducción",
    "href": "posts/modulos/index.html#introducción",
    "title": "Usando módulos de torch",
    "section": "Introducción",
    "text": "Introducción\nInicialmente, empezamos aprendiendo sobre las bases de torch programando una red neuronal sencilla “desde el principio”, haciendo uso solamente de una de las características de torch: los tensores. Ahora, vamos a simplificar muchísimo la tarea reemplazando la retro-propagación con autograd. Hoy vamos a modularizar la red en dos sentidos: en el habitual y en uno mucho mas literal: las operaciones de matrices de bajo nivel son reemplazadas por módulos de torch."
  },
  {
    "objectID": "posts/optimizadores/index.html",
    "href": "posts/optimizadores/index.html",
    "title": "Optimizadores en torch",
    "section": "",
    "text": "Esta es la cuarta y última entrega de una serie que presenta las bases de torch. Inicialmente, no enfocamos en los tensores. Para ilustrar su potencia, codificamos una red neuronal completa (aunque de pequeño tamaño) desde cero. Allí no se usó ninguna de las capacidades de alto nivel de torch, ni siquiera autograd, su herramienta de diferenciación automática.\nEsto cambio en la siguiente entrega. No seguimos pensando en derivadas o en la regla de la cadena; un llamado a backward() fue suficiente.\nEn la tercera entrega, el código vio nuevamente una simplificación importante. En lugar del tedioso ensamble del grafo (disposición de las capas) manualmente, se dejó que los módulos se encargaran.\nPartiendo de lo anterior, quedan dos cosas mas por hacer. Primero, aún calculamos las perdidas a mano. Segundo, aunque obtenemos los gradientes buenamente calculados de autograd, aún programamos un ciclo sobre los parámetros para actualizarlos por nuestros propios medios. No es una sorpresa saber que nada de esto es necesario."
  },
  {
    "objectID": "posts/optimizadores/index.html#perdidas-y-funciones-de-perdidas",
    "href": "posts/optimizadores/index.html#perdidas-y-funciones-de-perdidas",
    "title": "Optimizadores en torch",
    "section": "Perdidas y funciones de perdidas",
    "text": "Perdidas y funciones de perdidas\ntorch incluye todas las funciones usuales de perdidas, tales como error cuadrático medio, entropía cruzada, divergencia Kullback-Leibler y similares. En general, hay dos modos de uso.\nTomemos por ejemplo el calculo del error cuadratico medio. Una manera es invocando nnf_mse_loss() directamente en la predicción y los valores de salida verdaderos:\n\nlibrary(torch)\n\nx <- torch_randn(c(3, 2, 3))\ny <- torch_zeros(c(3, 2, 3))\n\nnnf_mse_loss(x, y)\n\ntorch_tensor\n1.20262\n[ CPUFloatType{} ]\n\n\nOtras funciones de perdidas designadas para ser invocadas directamente inician con nnf_ como: nnf_binary_cross_entropy(), nnf_nll_loss(), nnf_kl_div() y asi sucesivamente 1.\nLa segunda forma es definir el algoritmo previamente e invocarlo posteriormente. En este caso, todos los constructores inician con nn_ y terminan en _loss. Por ejemplo: nn_bce_loss(), nn_nll_loss(), nn_kl_div_loss(), etc 2.\n\nloss <- nn_mse_loss()\n\nloss(x, y)\n\ntorch_tensor\n1.20262\n[ CPUFloatType{} ]\n\n\nEl último método es preferido cuando el mismo único algoritmo debe ser aplicado a mas de un par de tensores."
  },
  {
    "objectID": "posts/optimizadores/index.html#optimizadores",
    "href": "posts/optimizadores/index.html#optimizadores",
    "title": "Optimizadores en torch",
    "section": "Optimizadores",
    "text": "Optimizadores\nHasta ahora, hemos estado actualizando los parámetros del modelo usando una estrategia simple: Los gradientes nos indican en que dirección la curva de la función de perdidas va hacia abajo; la rata de aprendizaje nos dice que tan grande debe ser el paso que se de en dicha dirección. Lo que hicimos fue una implementación directa del descenso del gradiente.\nSin embargo, los algoritmos de optimización usado en aprendizaje profundo son mucho mas sofisticados. Abajo, observaremos como reemplazar nuestras actualizaciones manuales usando el algoritmo Adam (Kingma y Ba 2017). Aunque primero, demos un vistazo a cómo trabajan los optimizadores de torch.\nAquí tenemos una red muy sencilla que consiste en una sola capa lineal a ser invocada por un único punto de datos (una salida).\n\ndata <- torch_randn(1, 3)\n\nmodel <- nn_linear(3, 1)\nmodel$parameters\n\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nCuando se crea un optimizador, le estamos diciendo que parámetros deben ser usados.\n\noptimizer <- optim_adam(model$parameters, lr = 0.01)\noptimizer\n\n<optim_adam>\n  Inherits from: <torch_optimizer>\n  Public:\n    add_param_group: function (param_group) \n    clone: function (deep = FALSE) \n    defaults: list\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    load_state_dict: function (state_dict) \n    param_groups: list\n    state: State, R6\n    state_dict: function () \n    step: function (closure = NULL) \n    zero_grad: function () \n  Private:\n    step_helper: function (closure, loop_fun) \n\n\nEn cualquier momento podemos inspeccionar estos parámetros:\n\noptimizer$param_groups[[1]]$params\n\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nAhora vamos a realizar la propagación hacia adelante y hacia atrás. La retro-propagación calculará los gradientes, pero no actualiza los parámetros, como podemos ver a continuación de los objetos model y optimizer:\n\nout <- model(data)\nout$backward()\n\noptimizer$param_groups[[1]]$params\n\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\nmodel$parameters\n\n$weight\ntorch_tensor\n 0.4308  0.2939  0.4302\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5542\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nInvocando el método step() en el optimizador se realiza la actualización de los pesos del modelo. De nuevo, revisemos que, tanto model como optimizer, ahora contienen los valores actualizados:\n\noptimizer$step()\n\nNULL\n\noptimizer$param_groups[[1]]$params\n\n$weight\ntorch_tensor\n 0.4208  0.3039  0.4202\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5642\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\nmodel$parameters\n\n$weight\ntorch_tensor\n 0.4208  0.3039  0.4202\n[ CPUFloatType{1,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n-0.5642\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nSi realizamos la optimización en un ciclo, necesitamos asegurarnos de que la invocación a `optimizer$zero_grad() en cada paso, porque de otro modo los gradientes se acumularían. Podemos ahora ver la versión final de nuestra red neuronal."
  },
  {
    "objectID": "posts/optimizadores/index.html#red-neuronal-simple-la-version-final",
    "href": "posts/optimizadores/index.html#red-neuronal-simple-la-version-final",
    "title": "Optimizadores en torch",
    "section": "Red neuronal simple: la version final",
    "text": "Red neuronal simple: la version final\n\n### generación de datos de entrenamiento ---------------\n\n# dimensiones de la entrada (número de características de entrada)\nd_in <- 3\n# dimensiones de la salida (número de características de predicción)\nd_out <- 1\n# número de observaciones en el conjunto de entrenamiento\nn <- 100\n\n# Creación de datos aleatorios\nx <- torch_randn(n, d_in)\ny <- x[, 1, drop = F] * 0.2 - x[, 2, drop = F] * 1.3 - x[, 3, drop = F] * 0.5 + torch_randn(n, 1)\n\n### Definición de la red neuronal\n\n# dimensiones de la capa oculta\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### Parámetros de la red\n\n# para optimización Adam, necesitamos escoger una tasa de aprendizaje mas alta en este caso\nlearning_rate <- 0.08\n\noptimizer <- optim_adam(model$parameters, lr = learning_rate)\n\n### Ciclo de entrenamiento\n\nfor (t in 1:200){\n  ### ------ propagación hacia adelante---------\n  \n  y_pred <- model(x)\n  \n  ### ------ cálculo de perdidas --------\n  \n  loss <- nnf_mse_loss(y_pred, y, reduction = \"sum\")\n  if(t %% 10 ==0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### ------- retro-propagación ---------\n  \n  # puesta a cero de los gradientes antes de iniciar la retro-propagación\n  optimizer$zero_grad()\n  \n  # Cálculo de los gradientes para los parámetros del modelo\n  loss$backward()\n  \n  ### ------- actualización de los pesos -------\n  \n  # se usa el optimizador para actualizar los parámetros del modelo\n  optimizer$step()\n}\n\nEpoch:  10    Loss:  101.9358 \nEpoch:  20    Loss:  83.14806 \nEpoch:  30    Loss:  76.63703 \nEpoch:  40    Loss:  71.65172 \nEpoch:  50    Loss:  65.41241 \nEpoch:  60    Loss:  59.46485 \nEpoch:  70    Loss:  55.67398 \nEpoch:  80    Loss:  52.05198 \nEpoch:  90    Loss:  50.23497 \nEpoch:  100    Loss:  48.91325 \nEpoch:  110    Loss:  45.77558 \nEpoch:  120    Loss:  44.74579 \nEpoch:  130    Loss:  41.58841 \nEpoch:  140    Loss:  49.05213 \nEpoch:  150    Loss:  42.33701 \nEpoch:  160    Loss:  38.36844 \nEpoch:  170    Loss:  38.33907 \nEpoch:  180    Loss:  36.47657 \nEpoch:  190    Loss:  35.86893 \nEpoch:  200    Loss:  36.88996"
  }
]